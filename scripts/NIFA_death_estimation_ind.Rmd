---
title: "NIFA - death estimations"
author: "Cassandra Wattenburger"
date: "1/13/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
```

```{r}
# Clear working directory, load in packages, generate package info
rm(list=ls())

library("tidyverse")
library("trelliscopejs")

sessionInfo()
```

# Import data

* Normalized count data
* Taxonomy table
* Metadata

```{r}
# Count data
norm <- readRDS("../data_intermediate/NIFA_normalized.rds")

# Taxonomy
tax <- read_tsv(file="../data_amplicon/NIFA2/final/NIFA2.taxonomy-final.tsv")

# Metadata
meta <- read_tsv("../NIFA_metadata.tsv")

dim(norm)
```

# Prepare data

Remove non-present taxa:

```{r}
norm_prep <- filter(norm, norm_abund != 0) 
  
dim(norm_prep)
```

Remove 13C samples:

```{r}
norm_prep <- filter(norm_prep, Isotope=="12C")
```

Remove taxa that didn't occur in at least five time points:

```{r}
# Identify ASVs that appeared less than 4 times in time series
occurences <- norm_prep %>% 
  group_by(ASV, Soil, Replicate) %>% 
  summarize(occurs = n()) %>% 
  filter(occurs > 4) %>% 
  ungroup()

# Filter
norm_prep <- inner_join(norm_prep, occurences, by=c("ASV", "Soil", "Replicate")) %>% 
  select(everything(), -occurs)

dim(norm_prep)
```

Natural log transform:

```{r}
norm_prep <- norm_prep %>% 
  mutate(ln_norm_abund = log(norm_abund))
```

Create unique label for each ASV and time series:

```{r}
norm_prep <- norm_prep %>%
  mutate(label = paste0(Soil, Replicate, "_", ASV))
```

# Estimate death

Prepare data:

```{r}
# Rename columns to match algorithm reqiurements
norm_prepped <- norm_prep %>% 
  select(label, Soil, Replicate, time=Day, ASV, abund=ln_norm_abund) %>% 
  arrange(time) # order by time
```

Create growth parameter saving function:

```{r}
# Function: Saves chosen growth estimate from below algorhithm
save_fit <- function(label, start, end, df_sub, output) {
  # Save estimate info
  est <- NULL; coeff <- NULL; yint <- NULL; residuals <- NULL; pval <- NULL; thisrow <- data.frame() # clear previous
  est <- lm(abund ~ time, data=df_sub[start:end,])
  coeff <- as.numeric(est$coefficients[2])
  yint <- as.numeric(est$coefficients[1])
  residuals <- sum(abs(resid(est)))
  pval <- summary(est)$coefficients[2,4]
  thisrow <- data.frame(label, start, end, coeff, yint, pval, residuals)
  output <- bind_rows(output, thisrow)
  return(output)
}
```

```{r, eval=FALSE}
#Testing subset for faster troubleshooting
label_test <- c(unique(norm_prepped$label)[1:100], unique(norm_prepped$label)[4800:4900])

norm_test <- filter(norm_prepped, label %in% label_test)
```

Estimate:

SLOW STEP

```{r, eval=FALSE}
# Estimate death

## Requires a dataframe with columns containing a unique label for each data point, abundance values, time points
## df = data frame containing time series with abundance values (ln transformed), long format
## df must contain columns named:
### label = column with unique identifier for each time series
### abund = column with abundance values at each time point
### time = column with time point values
df <- norm_prepped
death_estimates <- data.frame()
for (label in as.character(unique(df$label))) {

  # Subset one time series using the label
  df_sub <- data.frame()
  df_sub <- df[df$label==label,] 
  stop <- FALSE
    
  # Sliding window (at least four time points in window)
  for (start in 1:(nrow(df_sub) - 3)) { # don't bother with end of timeseries estimations less than 4 tp
    stop <- FALSE
    for (end in (start + 3):nrow(df_sub)) {
      if (stop == TRUE) {break}
     
      # Fit linear model to the window
      window_lm <- NULL; window_p <- NULL; window_coeff <- NULL
      window_lm <- lm(abund ~ time, data = df_sub[start:end,])
      window_p <- summary(window_lm)$coefficients[2,4]
      window_coeff <- window_lm$coefficients[2]
  
      # If a suitable fit is found and more time points exist that were not included, try extending the window
      if (window_p <= 0.05 & window_coeff < 0 & end < nrow(df_sub)) {
        for (extend_end in ((end+1):nrow(df_sub))) {
          if (stop == TRUE) {break}
            
          # Fit linear model to previous, non-extended window
          prevwindow_lm <- NULL; prevwindow_p <- NULL; prevwindow_coeff <- NULL 
          prevwindow_lm <- lm(abund ~ time, data = df_sub[start:(extend_end - 1),])
          prevwindow_p <- summary(prevwindow_lm)$coefficients[2,4]
          prevwindow_coeff <- prevwindow_lm$coefficients[2]
            
          # Fit linear model to the extended window
          newwindow_lm <- NULL; newwindow_p <- NULL; newwindow_coeff <- NULL 
          newwindow_lm <- lm(abund ~ time, data = df_sub[start:extend_end,])
          newwindow_p <- summary(newwindow_lm)$coefficients[2,4]
          newwindow_coeff <- newwindow_lm$coefficients[2]
            
          # If see improvement and can add more data, continue extending the window
          if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end < nrow(df_sub)) {
            next
          }
            
          # If no improvement, save the previous fit
          else if (newwindow_p > prevwindow_p & prevwindow_coeff < 0 & extend_end < nrow(df_sub)) {
            end <- extend_end - 1
            death_estimates <- save_fit(label, start, end, df_sub, death_estimates)
            stop <- TRUE
          }
            
          # If see improvement (or no harm) but no more data points to fit, save the extended fit
          else if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end == nrow(df_sub)) {
            end <- extend_end
            death_estimates <- save_fit(label, start, end, df_sub, death_estimates)
            stop <- TRUE
          } 
        }
      }
        
      # If no more data available to add to model, save the fit
      else if (window_p <= 0.05 & window_coeff < 0 & end == nrow(df_sub)) {
        death_estimates <- save_fit(label, start, end, df_sub, death_estimates)
        stop <- TRUE
      }
    }
  }
}

dim(death_estimates)
```

```{r, eval=FALSE}
saveRDS(death_estimates, "../data_intermediate/NIFA_dests_raw_ind.rds")
```

```{r}
# Load raw estimates
death_estimates <- readRDS("../data_intermediate/NIFA_dests_raw_ind.rds")
```

### Quality filter

* Remove essentially perfect fits
* Remove pvalue > 0.05
* Remove slope < 0

Note: I can't find any guidance on how to determine whether or not a fit is "perfect" but I know that the residuals are essentially equal to 0 for perfect fits. I'll use 0.0001 as a filtering threshold for removal.

```{r}
# Remove "perfect" fits as precaution
death_estimates <- filter(death_estimates, residuals >= 0.0001)
dim(death_estimates)

# Remove any p-values > 0.05 for safety
death_estimates <- filter(death_estimates, pval < 0.05)
dim(death_estimates)

# Remove positive slopes for safety
death_estimates <- filter(death_estimates, coeff < 0)
dim(death_estimates)

# I noticed on a previous run-through that short (4 time points) estimates are spurious, remove those
death_estimates <- death_estimates %>% 
  mutate(length = end-start+1) %>% 
  filter(length > 4)

dim(death_estimates)
```

### Select best fit for each time series

* Lowest p-value of the slope

```{r}
# Choose lowest p-value window
lowest_pvals <- death_estimates %>% 
  group_by(label) %>% 
  summarize(pval = min(pval)) %>% 
  ungroup()

dim(lowest_pvals)

# Filter chosen estimates
death_estimates <- death_estimates %>% 
  semi_join(lowest_pvals)

dim(death_estimates)
```

# False positive control

Histogram of quality filtered p-values from actual estimates:

```{r}
hist(death_estimates$pval, xlab="P-values", main="Histogram of quality filtered p-values")
```

See: http://varianceexplained.org/statistics/interpreting-pvalue-histogram/

I'm choosing to use a permutation approach where I use my growth estimating algorhithm on randomly generated data with characteristics of the real data. I'll use this false positive information to filter my real estimates. Traditional false positive control methods are far too conservative for my dataset.

### Simulate random data

Use data simulated in NIFA_growth_estimation_ind.Rmd

```{r, eval=FALSE}
sim_data <- readRDS("../data_intermediate/NIFA_simulated_ind.rds")
```

The number of parameters variable is off from real data, even though I chose directly from it? Why?

### Estimate "death" on simulated data

Same algorithm.

```{r, eval=FALSE}
# Prep simulated data (rename columns)
sim_prepped <- sim_data %>% 
  select(label=simulation, abund=rand_abund, time=rand_day)

# Run algorithinmmmnmnm
df <- sim_prepped
sim_estimates <- data.frame()
for (label in as.character(unique(df$label))) {
    
  # Subset one time series using the label
  df_sub <- data.frame()
  df_sub <- df[df$label==label,] 
  stop <- FALSE
    
  # Sliding window
  for (start in 1:(nrow(df_sub) - 3)) {
    stop <- FALSE
    for (end in (start + 3):nrow(df_sub)) {
      if (stop == TRUE) {break}
     
      # Fit linear model to the window
      window_lm <- NULL; window_p <- NULL; window_coeff <- NULL
      window_lm <- lm(abund ~ time, data = df_sub[start:end,])
      window_p <- summary(window_lm)$coefficients[2,4]
      window_coeff <- window_lm$coefficients[2]
  
      # If a suitable fit is found and more time points exist that were not included, try extending the window
      if (window_p <= 0.05 & window_coeff < 0 & end < nrow(df_sub)) {
        for (extend_end in ((end+1):nrow(df_sub))) {
          if (stop == TRUE) {break}
            
          # Fit linear model to previous, non-extended window
          prevwindow_lm <- NULL; prevwindow_p <- NULL; prevwindow_coeff <- NULL 
          prevwindow_lm <- lm(abund ~ time, data = df_sub[start:(extend_end - 1),])
          prevwindow_p <- summary(prevwindow_lm)$coefficients[2,4]
          prevwindow_coeff <- prevwindow_lm$coefficients[2]
            
          # Fit linear model to the extended window
          newwindow_lm <- NULL; newwindow_p <- NULL; newwindow_coeff <- NULL 
          newwindow_lm <- lm(abund ~ time, data = df_sub[start:extend_end,])
          newwindow_p <- summary(newwindow_lm)$coefficients[2,4]
          newwindow_coeff <- newwindow_lm$coefficients[2]
            
          # If see improvement and can add more data, continue extending the window
          if (newwindow_p <= prevwindow_p & newwindow_coeff > 0 & extend_end < nrow(df_sub)) {
            next
          }
            
          # If no improvement, save the previous fit
          else if (newwindow_p > prevwindow_p & prevwindow_coeff < 0 & extend_end < nrow(df_sub)) {
            end <- extend_end - 1
            sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
            stop <- TRUE
          }
            
          # If see improvement (or no harm) but no more data points to fit, save the extended fit
          else if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end == nrow(df_sub)) {
            end <- extend_end
            sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
            stop <- TRUE
          } 
        }
      }
        
      # If no more data available to add to model, save the fit
      else if (window_p <= 0.05 & window_coeff < 0 & end == nrow(df_sub)) {
        sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
        stop <- TRUE
      }
    }
  }
}
dim(sim_estimates)
```

Save simulated estimates:

```{r, eval=FALSE}
saveRDS(sim_estimates, "../data_intermediate/NIFA_simests_death_raw_ind.Rmd")
```

```{r}
sim_estimates <- readRDS("../data_intermediate/NIFA_simests_death_raw_ind.Rmd")
```

### Quality filter

* Remove essentially perfect fits
* Remove pvalue > 0.05
* Remove slope < 0
* remove length < 5

Note: I can't find any guidance on how to determine whether or not a fit is "perfect" but I know that the residuals are essentially equal to 0 for perfect fits. I'll use 0.0001 as a filtering threshold for removal.

```{r}
# Remove "perfect" fits as precaution
sim_estimates <- filter(sim_estimates, residuals >= 0.0001)
dim(sim_estimates)

# Remove any p-values > 0.05 
sim_estimates <- filter(sim_estimates, pval < 0.05)
dim(sim_estimates)

# Remove negative slopes for safety
sim_estimates <- filter(sim_estimates, coeff < 0)
dim(sim_estimates)

# Remove short (4 tps) estimates
sim_estimates <- sim_estimates %>% 
  mutate(length = end-start+1) %>% 
  filter(length > 4)

dim(sim_estimates)
```

### Select best fit for each time series

* Lowest p-value of the slope

```{r}
# Choose lowest p-value window
sim_lowest_pvals <- sim_estimates %>% 
  group_by(label) %>% 
  summarize(pval = min(pval)) %>% 
  ungroup()

dim(sim_lowest_pvals)

# Filter chosen estimates
sim_estimates <- sim_estimates %>% 
  semi_join(sim_lowest_pvals)

dim(sim_estimates)
```

False positive rates:

```{r}
# False positives
a <- nrow(sim_estimates[sim_estimates$pval <= 0.05,])
b <- nrow(sim_estimates[sim_estimates$pval <= 0.025,])
c <- nrow(sim_estimates[sim_estimates$pval <= 0.01,])
d <- nrow(sim_estimates[sim_estimates$pval <= 0.005,])
e <- nrow(sim_estimates[sim_estimates$pval <= 0.001,])
f <- nrow(sim_estimates[sim_estimates$pval <= 0.0005,])

false_pos <- data.frame(c(0.05, 0.025, 0.01, 0.005, 0.001, 0.0005), c(a,b,c,d,e,f))
colnames(false_pos)=c("pvalue","false")

ggplot(false_pos, aes(x=pvalue, y=false)) +
  geom_point() +
  geom_smooth(method="lm", linetype=2) +
  labs(title="Relationship between p-value and number of false positives", x="P-value", y="False positives") +
  theme_test()
```

Find 10%, 5%, and 1% false positives:

```{r}
# Guess and check
nrow(sim_estimates[sim_estimates$pval <= 0.0094,]) # ~10% (100/1000 estimates)
nrow(sim_estimates[sim_estimates$pval <= 0.0041,]) # ~5% (50/1000 estimates)
nrow(sim_estimates[sim_estimates$pval <= 0.0006,]) # ~1% (10/1000 estimates) 

false10_pval <- 0.0094
false5_pval <- 0.0041
false1_pval <- 0.0006
```

### Filter by false positive rate

Number of estimates in each category:

```{r, restuls="show"}
# ~10% false positives
death_falsepos10 <- subset(death_estimates, pval <= false10_pval)
nrow(death_falsepos10)

# ~5% false positives
death_falsepos5 <- subset(death_estimates, pval <= false5_pval)
nrow(death_falsepos5)

# 1%
death_falsepos1 <- subset(death_estimates, pval <= false1_pval)
nrow(death_falsepos1)
```

# Choose FPR

10% false positive rate:

```{r}
set.seed(100)

# 10% false positive rate
labels10 <- as.character(death_falsepos10$label)

# Randomly plot 10 w/growth estimate
for (n in 1:10) {
  # Choose randomly
  rand_num <- sample(1:nrow(death_falsepos10), 1)
  l <- labels10[rand_num]
  
  # Estimated growth window
  start <- death_falsepos10 %>% 
    filter(label==l) %>% 
    .$start
  
  end <- death_falsepos10 %>% 
    filter(label==l) %>% 
    .$end
  
  # Plot
  data_sub <- norm_prepped %>% 
    filter(label == l)
  plot <- data_sub %>% 
    ggplot(aes(x=time, y=abund)) +
    geom_smooth(method="lm", data=data_sub[start:end,], linetype=2) +
    geom_point() +
    geom_line() +
    theme_test()
  print(plot)
}
    
```

5% false positive rate:

```{r}
labels5 <- as.character(death_falsepos5$label)

# Randomly plot 10 w/growth estimate
for (n in 1:10) {
  # Choose randomly
  rand_num <- sample(1:nrow(death_falsepos5), 1)
  l <- labels5[rand_num]
  
  # Estimated growth window
  start <- death_falsepos5 %>% 
    filter(label==l) %>% 
    .$start
  
  end <- death_falsepos5 %>% 
    filter(label==l) %>% 
    .$end
  
  # Plot
  data_sub <- norm_prepped %>% 
    filter(label == l)
  plot <- data_sub %>% 
    ggplot(aes(x=time, y=abund)) +
    geom_smooth(method="lm", data=data_sub[start:end,], linetype=2) +
    geom_point() +
    geom_line() +
    theme_test()
  print(plot)
}
```

# Calculate death metrics

### Calculate specific death rate (k)

Steps:

* Use estimated slope from growth curves to simulate growth over a time interval
* Use simulated abundances to solve for k

Using estimates filtered at 10% false positives.

Formula: k=(log10(b)-log10(B))*2.303/t

Where k is specific growth rate, B is abundance at beginning, b is abundance at end, and t is the time interval.

I found this guide helpful: http://miller-lab.net/MillerLab/protocols/general-bacteriology/calculating-growth-rate/

```{r}
# Generate abundances over time interval based on estimated slope
death_estimates2 <- data.frame()
B <- 1 # Start with one bacteria
for (l in death_falsepos10$label) {
  death_label <- filter(death_falsepos10, label==l)
  coeff <- abs(death_label$coeff) # remove negative
  b <- (coeff*3)+1 # abundance three days later
  k <- abs((log10(b)-log10(B)))*(2.303/3) # calculate k, absolute value to get rid of negative
  this_row <- cbind(death_label, k)
  death_estimates2 <- rbind(death_estimates2, this_row)
}
```

### Start and end day, change in abundance

```{r}
# Convert start and end to actual day
death_final <- data.frame()
for (l in as.character(unique(death_estimates2$label))) {
  # Isolate timeseries
  norm_label <- norm_prep %>% 
    filter(label==l) %>% 
    arrange(Day)
  death_label <- filter(death_estimates2, label==l)
  start <- death_label$start
  end <- death_label$end
  # Start and end day of growth
  start_day <- norm_label[start,]$Day
  end_day <- norm_label[end,]$Day
  # Starting and ending relational abundance
  start_abund <- norm_label[start,]$norm_abund
  end_abund <- norm_label[end,]$norm_abund
  change_abund <- end_abund - start_abund
  # Save output
  this_row <- bind_cols(label = as.character(death_label$label), 
                        slope = death_label$coeff, yint = death_label$yint, k = death_label$k,
                        start_pt = death_label$start, end_pt = death_label$end,
                        start_day = start_day, end_day = end_day, 
                        start_abund = start_abund, end_abund = end_abund, change_abund = change_abund)
  death_final <- bind_rows(death_final, this_row)
}
```

## Halving times

Formula: h = ln2/k

```{r}
# Calculate doubling time based on k
h_df <- data.frame()
for (l in as.character(unique(death_final$label))) {
        data_sub <- filter(death_final, label==l)
        k <- data_sub$k
        h <- log(2)/k
        thisrow <- cbind(data_sub, h)
        h_df <- rbind(h_df, thisrow)
}

death_final2 <- h_df %>% 
  select(label, slope, yint, k, h, start_pt, end_pt, start_day, end_day, start_abund, end_abund, change_abund)

# Summary
mean(h_df$h)
sd(h_df$h)
min(h_df$h)
max(h_df$h)
```

# Tidy up and save data

Death estimates:

```{r}
# Add back metadata from label
death_tidy <- death_final2 %>% 
  # Label sturcture: (soil)(replicate)_(asv)
  mutate(Soil = gsub("([A|M][a-z]+)([0-3])_(.+)", "\\1", label),
         Replicate = gsub("([A|M][a-z]+)([0-3])_(.+)", "\\2", label), 
         Replicate = as.numeric(Replicate),
         ASV = gsub("([A|M][a-z]+)([0-3])_(.+)", "\\3", label)) %>%
  select(label, Soil, Replicate, ASV, slope:change_abund)

head(death_tidy)
```

```{r, eval=FALSE}
saveRDS(death_tidy, file="../data_intermediate/NIFA_dests_final_ind.rds")
```

# Summary of death estimates

```{r}
death_tidy <- readRDS("../data_intermediate/NIFA_dests_final_ind.rds")

# Number of estimates per replicate
death_tidy %>% 
  group_by(Soil, Replicate) %>% 
  summarize(total = n()) %>% 
  ungroup()

# Average estimates per soil
death_tidy %>% 
  group_by(Soil, Replicate) %>% 
  summarize(total=n()) %>% 
  ungroup() %>%
  group_by(Soil) %>% 
  summarize(total_mean = mean(total),
            total_sd = sd(total))

# Unique ASVs with estimates per soil
death_tidy %>% 
  group_by(Soil, Replicate) %>% 
  summarize(asvs = n()) %>% 
  ungroup() %>% 
  group_by(Soil) %>% 
  summarize(asvs_mean = mean(asvs),
            asvs_sd = sd(asvs))

# Phyla esitmates
death_tidy %>% 
  inner_join(tax) %>% # add taxonomy
  group_by(Soil, Replicate, Phylum) %>% 
  summarize(total = n()) %>% 
  ungroup() %>% 
  group_by(Soil, Phylum) %>% 
  summarize(total_mean = mean(total),
            total_sd = sd(total)) %>% 
  arrange(Soil, -total_mean)

# Average halving time per soil
death_tidy %>% 
  group_by(Soil, Replicate) %>% 
  summarize(h = mean(h)) %>% 
  ungroup() %>% 
  group_by(Soil) %>% 
  summarize(h_mean = mean(h),
            h_sd = sd(h))

death_tidy %>% 
  group_by(Soil, Replicate) %>% 
  summarize(h_mean = mean(h),
            h_sd = sd(h)) %>% 
  ggplot(aes(x=Soil, y=h_mean)) + 
  geom_point() +
  theme_test()

# Average lag time
death_tidy %>% 
  group_by(Soil, Replicate) %>% 
  summarize(lag = mean(start_day)) %>% 
  ungroup() %>% 
  group_by(Soil) %>% 
  summarize(lag_mean = mean(lag),
            lag_sd = sd(lag))

# Can't do change in abundance yet because need to correct for 16S copy num
```

# Plot death estimates

Selected randomly

```{r}
# Graph 30 random estimates
label_rand <- sample(unique(death_tidy$label), 30)
death_rand <- filter(death_tidy, label %in% label_rand)

count <- 0

for (l in as.character(death_rand$label)) {
  count <- count + 1
  # Subset time series
  death_label <- filter(death_rand, label==l)
  norm_label <- filter(norm_prep, label==l) %>% 
  arrange(Day)
  # Title information
  asv <- death_label$ASV
  tax_info <- filter(tax, ASV == asv)
  
  title <- paste0(count, ". ", tax_info$Phylum, ", ", tax_info$Genus)
  # Graph with estimate
  graph <- ggplot(norm_label, aes(x=Day, y=ln_norm_abund)) +
    geom_point(shape=1, size=3, color="#6F7378") +
    geom_line(color="#6F7378") +
    geom_smooth(method="lm", data=norm_label[death_label$start_pt:death_label$end_pt,], linetype=2, color="black") +
    labs(title=title, x="day", y="ln norm abundance") +
    theme_test() +
    theme(title = element_text(size=18),
        axis.title = element_text(size=16),
        axis.text = element_text(size=14))
  
  # Print
  print(graph)
  }
```

```{r}
count <- 0

for (l in unique(as.character(death_tidy$label))) {
  count <- count + 1
  # Subset time series
  death_label <- filter(death_tidy, label==l)
  norm_label <- filter(norm_prep, label==l) %>% 
  arrange(Day)
  # Title information
  title <- paste0(count, ". ", l)
  
  # Graph with estimate
  graph <- ggplot(norm_label, aes(x=Day, y=ln_norm_abund)) +
    geom_point(shape=1, size=3, color="#6F7378") +
    geom_line(color="#6F7378") +
    geom_smooth(method="lm", data=norm_label[death_label$start_pt:death_label$end_pt,], linetype=2, color="black") +
    #labs(title=title, x="day", y="ln norm abundance") +
    labs(title=title, x="day", y="ln norm abund") +
    theme_test() +
    theme(title = element_text(size=18),
        axis.title = element_text(size=16),
        axis.text = element_text(size=14))
  
  # Print
  print(graph)
  
  # Save to pdf
  #ggsave(file=paste0("../figures/all_timeseries/", count, "_", label, "_west.png"), graph)
}
```

