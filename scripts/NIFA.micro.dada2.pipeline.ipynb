{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: NIFA\n",
    "\n",
    "Cassandra Wattenburger, 10/17/22\n",
    "\n",
    "### Notes:\n",
    "* QIIME2 v2021.4\n",
    "* Micro run to test internal standard concentrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline information\n",
    "\n",
    "### Pipeline to process raw sequences with DADA2 ###\n",
    "* Prep for import into QIIME2 (modify sequence IDs and combine two index files)\n",
    "* Import into QIIME2\n",
    "* Demultiplex\n",
    "* Denoise and merge with DADA2\n",
    "* Prepare ASV tables and representative sequences *(Note: sample names starting with a digit will break this step)*\n",
    "* Classify sequences\n",
    "* Construct phylogenetic tree\n",
    "* Export from QIIME2\n",
    "\n",
    "*100% Appropriated from the \"Atacama Desert Tutorial\" for QIIME2*\n",
    "\n",
    "### Pipeline can handle both 16S rRNA gene and ITS sequences ###\n",
    "* Tested on 515f and 806r\n",
    "* Tested on ITS1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start\n",
    "\n",
    "### Commands to install dependencies ####\n",
    "##### || QIIME2 and biopython ||\n",
    "QIIME2 is still actively in development with frequent new releases. Check for the most up-to-date version and use that.\n",
    "\n",
    "Install QIIME2: \n",
    "* <https://docs.qiime2.org/2017.11/install/native/#install-qiime-2-within-a-conda-environment>, follow the instructions to install QIIME2 in Linux (64-bit).\n",
    "\n",
    "Activate the QIIME2 environment:\n",
    "* source activate [qiime2-pipeline-name]\n",
    "\n",
    "After you install and activate the QIIME2 environment, you must also install biopython for the barcode concatenation step to work. To install biopython make sure the QIIME2 environment is activated and run:\n",
    "* conda install -c anaconda biopython\n",
    "\n",
    "Install cutadapt to the QIIME2 environment as well. Cutadapt removes primers from the sequences.\n",
    "* conda install -c bioconda cutadapt\n",
    "\n",
    "##### || Copyrighter rrn database ||\n",
    "The script will automatically install the curated GreenGenes rrn attribute database: https://github.com/fangly/AmpliCopyrighter\n",
    "\n",
    "#### Citations\n",
    "* Caporaso, J. G., Kuczynski, J., Stombaugh, J., Bittinger, K., Bushman, F. D., Costello, E. K., *et al.* (2010). QIIME allows analysis of high-throughput community sequencing data. Nature methods, 7(5), 335-336.\n",
    "\n",
    "* Angly, F. E., Dennis, P. G., Skarshewski, A., Vanwonterghem, I., Hugenholtz, P., & Tyson, G. W. (2014). CopyRighter: a rapid tool for improving the accuracy of microbial community profiles through lineage-specific gene copy number correction. Microbiome, 2(1), 11.\n",
    "\n",
    "### Using jupyter notebook screens ###\n",
    "\n",
    "With the QIIME2 environment activated, open your jupyter notebook screen in the directory containing this script:\n",
    "* jupyter-n [screen-name] [port #]\n",
    "\n",
    "See [these instructions](https://github.com/buckleylab/Buckley_lab_protocols/blob/master/Using_the_server/getting_started_on_server.md#make-jupyter-notebook-screens-command) for how to set up and use this command on the server.\n",
    "\n",
    "### Directory and data organization ###\n",
    "\n",
    "This pipeline assumes that you've organized your data in a certain way:\n",
    "* Each library of raw data is contained in a separate directory\n",
    "* Each library has a separate working directory within a larger project directory\n",
    "* Tree construction assumes all 16S libraries processed together will be analyzed together, so one tree is made based on all 16S libraries and is placed in a separate tree directory within the project directory\n",
    "\n",
    "### Troubleshooting tip ###\n",
    "Replace os.system with print, and copy/paste the output into the command line to view the error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: User Input\n",
    "\n",
    "Metadata requirements:\n",
    "* Must be located in each library directory\n",
    "* Must be .tsv format \n",
    "* First column is named \"SampleID\" with sample names as rows\n",
    "* Contains another column named \"BarcodeSequence\" with the relevant barcode seqeunces as rows (rev. comp. reverse barcode sequence concatenated with forward barcode sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np\n",
    "\n",
    "# Prepare an object with the name of the library and all related file paths\n",
    "# datasets = [['library name prefix', 'processed data directory path', 'raw data directory', 'read1 file name', 'read2 file name', 'index1 file name', 'index2 file name', 'metadata file name', 'domain of life (bacteria or fungi)'], ...]\n",
    "project = \"/home/cassi/NIFA/data_amplicon/NIFA_micro\" # this can be the same as the library directory if you only have one library to process\n",
    "libraries = [['NIFA_micro', \n",
    "             '/home/cassi/NIFA/data_amplicon/NIFA_micro', \n",
    "             '/home/backup_files/raw_reads/NIFA.micro.Cassi.2022',\n",
    "             '174800_GDY2V_NIFA_micro_S1_R1_001.fastq.gz', \n",
    "             '174800_GDY2V_NIFA_micro_S1_R2_001.fastq.gz', \n",
    "             '174800_GDY2V_NIFA_micro_S1_I1_001.fastq.gz', \n",
    "             '174800_GDY2V_NIFA_micro_S1_I2_001.fastq.gz', \n",
    "             'NIFA_micro_demultiplex.tsv',\n",
    "             'bacteria']]\n",
    "\n",
    "# Set # of processors\n",
    "processors = 10\n",
    "\n",
    "# Which bacterial database will you use? Silva or GreenGenes\n",
    "db = \"Silva\"\n",
    "\n",
    "# Phylogenetic tree (non-fungal data)\n",
    "treename = \"NIFA\" # name prefix for the tree file\n",
    "\n",
    "## Enter minimum support for keeping QIIME classification\n",
    "# Note: Classifications that do not meet this criteria will be retained, labeled 'putative'\n",
    "min_support = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Truncate sequence identifiers\n",
    "\n",
    "This step removes a portion at the end of the sequence ID that is incompatible with QIIME2. It will also create a modified directory in your raw data directory to house the modified data. The original raw data will not be modified, only the copies.\n",
    "\n",
    "Slow step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    raw = library[2]\n",
    "    read1 = library[3]\n",
    "    read2 = library[4]\n",
    "    index1 = library[5]\n",
    "    index2 = library[6]\n",
    "    \n",
    "    # Create a directory to place modified sequence data\n",
    "    if not os.path.isdir(os.path.join(raw, \"modified\")):\n",
    "        !mkdir $raw/modified\n",
    "    \n",
    "    # Copy/paste all raw sequence data into modified directory\n",
    "    !cp $raw/*.fastq.gz $raw/modified\n",
    "    \n",
    "    # Decompress all files\n",
    "    !unpigz $raw/modified/*.fastq.gz\n",
    "    \n",
    "    # Decompressed file names\n",
    "    read1decomp = re.sub(\".fastq.gz\", \".fastq\", read1)\n",
    "    read2decomp = re.sub(\".fastq.gz\", \".fastq\", read2)\n",
    "    index1decomp = re.sub(\".fastq.gz\", \".fastq\", index1)\n",
    "    index2decomp = re.sub(\".fastq.gz\", \".fastq\", index2)\n",
    "    \n",
    "    # Remove problematic part of sequence IDs\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$read1decomp > $raw/modified/read1_mod.fastq\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$read2decomp > $raw/modified/read2_mod.fastq\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$index1decomp > $raw/modified/index1_mod.fastq\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$index2decomp > $raw/modified/index2_mod.fastq\n",
    "    \n",
    "    # Delete file copies\n",
    "    !rm $raw/modified/$read1decomp\n",
    "    !rm $raw/modified/$read2decomp\n",
    "    !rm $raw/modified/$index1decomp\n",
    "    !rm $raw/modified/$index2decomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Remove primer from sequences\n",
    "\n",
    "There may still be portions of the primers left in the read sequences that need to be removed. Use cutadapt to remove those portions. Read1 will have the reverse complement of the reverse primer and read2 will have the reverse complement of the forward primer in some sequences on the 3' end.\n",
    "\n",
    "You will get a warning that the adapter is preceded by \"A\" or \"G\" extremely often. These are the link sequences in the primer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is cutadapt 3.4 with Python 3.8.8\n",
      "Command line parameters: -a ATTAGAWACCCBDGTAGTCC -o /home/backup_files/raw_reads/NIFA.micro.Cassi.2022/modified/read1_noprimer.fastq /home/backup_files/raw_reads/NIFA.micro.Cassi.2022/modified/read1_mod.fastq\n",
      "Processing reads on 1 core in single-end mode ...\n",
      "[-->8        ] 00:00:39     3,897,911 reads  @     10.2 µs/read;   5.86 M reads/minute\n",
      "Finished in 39.92 s (10 µs/read; 5.86 M reads/minute).\n",
      "\n",
      "=== Summary ===\n",
      "\n",
      "Total reads processed:               3,897,911\n",
      "Reads with adapters:                    23,665 (0.6%)\n",
      "Reads written (passing filters):     3,897,911 (100.0%)\n",
      "\n",
      "Total basepairs processed:   978,375,661 bp\n",
      "Total written (filtered):    978,078,481 bp (100.0%)\n",
      "\n",
      "=== Adapter 1 ===\n",
      "\n",
      "Sequence: ATTAGAWACCCBDGTAGTCC; Type: regular 3'; Length: 20; Trimmed: 23665 times\n",
      "\n",
      "No. of allowed errors:\n",
      "1-9 bp: 0; 10-19 bp: 1; 20 bp: 2\n",
      "\n",
      "Bases preceding removed adapters:\n",
      "  A: 16.9%\n",
      "  C: 22.3%\n",
      "  G: 43.5%\n",
      "  T: 17.1%\n",
      "  none/other: 0.1%\n",
      "\n",
      "Overview of removed sequences\n",
      "length\tcount\texpect\tmax.err\terror counts\n",
      "3\t12828\t60904.9\t0\t12828\n",
      "4\t3607\t15226.2\t0\t3607\n",
      "5\t892\t3806.6\t0\t892\n",
      "6\t517\t951.6\t0\t517\n",
      "7\t210\t237.9\t0\t210\n",
      "8\t251\t59.5\t0\t251\n",
      "9\t243\t14.9\t0\t203 40\n",
      "10\t163\t3.7\t1\t138 25\n",
      "11\t140\t0.9\t1\t124 16\n",
      "12\t126\t0.2\t1\t119 7\n",
      "13\t87\t0.1\t1\t78 9\n",
      "14\t138\t0.0\t1\t126 12\n",
      "15\t106\t0.0\t1\t88 18\n",
      "16\t103\t0.0\t1\t91 12\n",
      "17\t89\t0.0\t1\t71 18\n",
      "18\t133\t0.0\t1\t100 24 9\n",
      "19\t114\t0.0\t1\t105 8 1\n",
      "20\t129\t0.0\t2\t105 13 11\n",
      "21\t92\t0.0\t2\t77 10 5\n",
      "22\t62\t0.0\t2\t54 4 4\n",
      "23\t311\t0.0\t2\t280 21 10\n",
      "24\t195\t0.0\t2\t171 11 13\n",
      "25\t83\t0.0\t2\t69 8 6\n",
      "26\t90\t0.0\t2\t74 8 8\n",
      "27\t184\t0.0\t2\t162 11 11\n",
      "28\t291\t0.0\t2\t246 27 18\n",
      "29\t172\t0.0\t2\t138 24 10\n",
      "30\t651\t0.0\t2\t562 66 23\n",
      "31\t121\t0.0\t2\t106 11 4\n",
      "32\t46\t0.0\t2\t41 3 2\n",
      "33\t63\t0.0\t2\t58 2 3\n",
      "34\t35\t0.0\t2\t23 8 4\n",
      "35\t37\t0.0\t2\t30 5 2\n",
      "36\t82\t0.0\t2\t68 6 8\n",
      "37\t48\t0.0\t2\t44 4\n",
      "38\t11\t0.0\t2\t10 1\n",
      "39\t10\t0.0\t2\t7 1 2\n",
      "40\t25\t0.0\t2\t20 3 2\n",
      "41\t20\t0.0\t2\t16 3 1\n",
      "42\t102\t0.0\t2\t89 10 3\n",
      "43\t25\t0.0\t2\t23 0 2\n",
      "44\t16\t0.0\t2\t15 1\n",
      "45\t22\t0.0\t2\t21 1\n",
      "46\t42\t0.0\t2\t35 2 5\n",
      "47\t72\t0.0\t2\t50 5 17\n",
      "48\t31\t0.0\t2\t30 1\n",
      "49\t9\t0.0\t2\t6 0 3\n",
      "50\t18\t0.0\t2\t17 1\n",
      "51\t10\t0.0\t2\t8 1 1\n",
      "52\t22\t0.0\t2\t17 3 2\n",
      "53\t6\t0.0\t2\t4 0 2\n",
      "54\t3\t0.0\t2\t3\n",
      "55\t11\t0.0\t2\t4 5 2\n",
      "56\t7\t0.0\t2\t4 3\n",
      "57\t7\t0.0\t2\t6 0 1\n",
      "58\t2\t0.0\t2\t0 1 1\n",
      "59\t7\t0.0\t2\t4 1 2\n",
      "60\t2\t0.0\t2\t0 2\n",
      "61\t62\t0.0\t2\t59 2 1\n",
      "63\t2\t0.0\t2\t0 0 2\n",
      "64\t1\t0.0\t2\t1\n",
      "65\t1\t0.0\t2\t0 1\n",
      "66\t8\t0.0\t2\t6 0 2\n",
      "67\t1\t0.0\t2\t0 1\n",
      "68\t1\t0.0\t2\t0 0 1\n",
      "70\t3\t0.0\t2\t1 2\n",
      "71\t4\t0.0\t2\t3 1\n",
      "72\t1\t0.0\t2\t0 1\n",
      "73\t9\t0.0\t2\t0 0 9\n",
      "75\t1\t0.0\t2\t0 0 1\n",
      "76\t6\t0.0\t2\t3 0 3\n",
      "77\t12\t0.0\t2\t9 2 1\n",
      "78\t6\t0.0\t2\t6\n",
      "83\t2\t0.0\t2\t0 1 1\n",
      "84\t9\t0.0\t2\t4 2 3\n",
      "85\t16\t0.0\t2\t13 2 1\n",
      "86\t8\t0.0\t2\t4 0 4\n",
      "87\t28\t0.0\t2\t21 2 5\n",
      "88\t1\t0.0\t2\t0 0 1\n",
      "91\t3\t0.0\t2\t0 3\n",
      "94\t1\t0.0\t2\t0 0 1\n",
      "95\t2\t0.0\t2\t1 1\n",
      "96\t2\t0.0\t2\t2\n",
      "97\t1\t0.0\t2\t0 0 1\n",
      "99\t6\t0.0\t2\t5 1\n",
      "100\t2\t0.0\t2\t1 1\n",
      "105\t4\t0.0\t2\t4\n",
      "106\t8\t0.0\t2\t4 2 2\n",
      "112\t10\t0.0\t2\t7 0 3\n",
      "113\t2\t0.0\t2\t0 2\n",
      "114\t8\t0.0\t2\t6 0 2\n",
      "115\t4\t0.0\t2\t1 3\n",
      "116\t2\t0.0\t2\t1 0 1\n",
      "117\t1\t0.0\t2\t0 0 1\n",
      "120\t2\t0.0\t2\t0 0 2\n",
      "121\t44\t0.0\t2\t10 0 34\n",
      "122\t2\t0.0\t2\t2\n",
      "123\t1\t0.0\t2\t0 1\n",
      "125\t3\t0.0\t2\t0 0 3\n",
      "127\t1\t0.0\t2\t0 1\n",
      "128\t1\t0.0\t2\t0 0 1\n",
      "129\t3\t0.0\t2\t0 2 1\n",
      "131\t6\t0.0\t2\t0 4 2\n",
      "132\t2\t0.0\t2\t0 2\n",
      "134\t1\t0.0\t2\t0 0 1\n",
      "135\t2\t0.0\t2\t1 1\n",
      "136\t1\t0.0\t2\t0 1\n",
      "137\t3\t0.0\t2\t2 1\n",
      "140\t5\t0.0\t2\t0 1 4\n",
      "141\t2\t0.0\t2\t0 1 1\n",
      "142\t2\t0.0\t2\t1 0 1\n",
      "143\t1\t0.0\t2\t0 1\n",
      "144\t21\t0.0\t2\t18 3\n",
      "146\t5\t0.0\t2\t0 4 1\n",
      "148\t1\t0.0\t2\t0 0 1\n",
      "150\t1\t0.0\t2\t1\n",
      "151\t1\t0.0\t2\t0 1\n",
      "152\t1\t0.0\t2\t0 0 1\n",
      "154\t1\t0.0\t2\t0 0 1\n",
      "157\t11\t0.0\t2\t9 0 2\n",
      "158\t2\t0.0\t2\t0 1 1\n",
      "159\t1\t0.0\t2\t0 1\n",
      "160\t1\t0.0\t2\t0 1\n",
      "161\t9\t0.0\t2\t8 0 1\n",
      "163\t6\t0.0\t2\t2 4\n",
      "164\t2\t0.0\t2\t0 2\n",
      "165\t1\t0.0\t2\t0 0 1\n",
      "168\t2\t0.0\t2\t0 2\n",
      "169\t1\t0.0\t2\t0 0 1\n",
      "170\t3\t0.0\t2\t2 1\n",
      "171\t1\t0.0\t2\t0 1\n",
      "172\t1\t0.0\t2\t0 1\n",
      "173\t1\t0.0\t2\t0 0 1\n",
      "174\t1\t0.0\t2\t1\n",
      "175\t1\t0.0\t2\t0 1\n",
      "177\t12\t0.0\t2\t9 2 1\n",
      "178\t2\t0.0\t2\t0 2\n",
      "179\t5\t0.0\t2\t0 1 4\n",
      "180\t1\t0.0\t2\t0 0 1\n",
      "181\t1\t0.0\t2\t0 0 1\n",
      "182\t2\t0.0\t2\t0 0 2\n",
      "183\t4\t0.0\t2\t0 0 4\n",
      "186\t3\t0.0\t2\t0 3\n",
      "187\t2\t0.0\t2\t0 2\n",
      "188\t2\t0.0\t2\t0 0 2\n",
      "189\t1\t0.0\t2\t0 0 1\n",
      "190\t2\t0.0\t2\t0 1 1\n",
      "191\t1\t0.0\t2\t0 1\n",
      "192\t19\t0.0\t2\t0 0 19\n",
      "196\t13\t0.0\t2\t0 1 12\n",
      "197\t1\t0.0\t2\t1\n",
      "198\t3\t0.0\t2\t0 1 2\n",
      "203\t1\t0.0\t2\t0 0 1\n",
      "205\t9\t0.0\t2\t1 8\n",
      "207\t91\t0.0\t2\t0 0 91\n",
      "208\t1\t0.0\t2\t0 0 1\n",
      "209\t1\t0.0\t2\t0 0 1\n",
      "211\t8\t0.0\t2\t5 3\n",
      "212\t4\t0.0\t2\t2 0 2\n",
      "214\t1\t0.0\t2\t1\n",
      "215\t1\t0.0\t2\t1\n",
      "216\t1\t0.0\t2\t0 1\n",
      "218\t5\t0.0\t2\t0 0 5\n",
      "219\t1\t0.0\t2\t0 1\n",
      "220\t18\t0.0\t2\t1 16 1\n",
      "222\t9\t0.0\t2\t0 8 1\n",
      "224\t2\t0.0\t2\t1 0 1\n",
      "225\t3\t0.0\t2\t0 0 3\n",
      "231\t1\t0.0\t2\t0 0 1\n",
      "232\t12\t0.0\t2\t3 5 4\n",
      "233\t2\t0.0\t2\t0 2\n",
      "234\t3\t0.0\t2\t1 1 1\n",
      "237\t3\t0.0\t2\t0 1 2\n",
      "238\t3\t0.0\t2\t0 1 2\n",
      "239\t6\t0.0\t2\t1 1 4\n",
      "240\t1\t0.0\t2\t0 1\n",
      "241\t10\t0.0\t2\t9 0 1\n",
      "245\t2\t0.0\t2\t0 0 2\n",
      "246\t4\t0.0\t2\t3 0 1\n",
      "247\t8\t0.0\t2\t4 4\n",
      "248\t28\t0.0\t2\t1 0 27\n",
      "249\t5\t0.0\t2\t2 3\n",
      "250\t11\t0.0\t2\t7 4\n",
      "251\t22\t0.0\t2\t0 19 3\n",
      "This is cutadapt 3.4 with Python 3.8.8\n",
      "Command line parameters: -a TTACCGCGGCKGCTGGCAC -o /home/backup_files/raw_reads/NIFA.micro.Cassi.2022/modified/read2_noprimer.fastq /home/backup_files/raw_reads/NIFA.micro.Cassi.2022/modified/read2_mod.fastq\n",
      "Processing reads on 1 core in single-end mode ...\n",
      "[-------=8   ] 00:00:31     3,897,911 reads  @      8.1 µs/read;   7.42 M reads/minute\n",
      "Finished in 31.54 s (8 µs/read; 7.42 M reads/minute).\n",
      "\n",
      "=== Summary ===\n",
      "\n",
      "Total reads processed:               3,897,911\n",
      "Reads with adapters:                    21,239 (0.5%)\n",
      "Reads written (passing filters):     3,897,911 (100.0%)\n",
      "\n",
      "Total basepairs processed:   978,375,661 bp\n",
      "Total written (filtered):    978,181,032 bp (100.0%)\n",
      "\n",
      "=== Adapter 1 ===\n",
      "\n",
      "Sequence: TTACCGCGGCKGCTGGCAC; Type: regular 3'; Length: 19; Trimmed: 21239 times\n",
      "\n",
      "No. of allowed errors:\n",
      "1-9 bp: 0; 10-19 bp: 1\n",
      "\n",
      "Bases preceding removed adapters:\n",
      "  A: 37.2%\n",
      "  C: 24.4%\n",
      "  G: 15.4%\n",
      "  T: 22.9%\n",
      "  none/other: 0.1%\n",
      "\n",
      "Overview of removed sequences\n",
      "length\tcount\texpect\tmax.err\terror counts\n",
      "3\t10968\t60904.9\t0\t10968\n",
      "4\t4278\t15226.2\t0\t4278\n",
      "5\t1346\t3806.6\t0\t1346\n",
      "6\t286\t951.6\t0\t286\n",
      "7\t270\t237.9\t0\t270\n",
      "8\t117\t59.5\t0\t117\n",
      "9\t178\t14.9\t0\t177 1\n",
      "10\t128\t3.7\t1\t93 35\n",
      "11\t115\t0.9\t1\t83 32\n",
      "12\t113\t0.2\t1\t80 33\n",
      "13\t73\t0.1\t1\t57 16\n",
      "14\t115\t0.0\t1\t68 47\n",
      "15\t79\t0.0\t1\t63 16\n",
      "16\t85\t0.0\t1\t70 15\n",
      "17\t70\t0.0\t1\t55 15\n",
      "18\t75\t0.0\t1\t33 42\n",
      "19\t88\t0.0\t1\t60 28\n",
      "20\t93\t0.0\t1\t75 18\n",
      "21\t73\t0.0\t1\t57 16\n",
      "22\t42\t0.0\t1\t31 11\n",
      "23\t245\t0.0\t1\t190 55\n",
      "24\t150\t0.0\t1\t50 100\n",
      "25\t67\t0.0\t1\t52 15\n",
      "26\t71\t0.0\t1\t35 36\n",
      "27\t142\t0.0\t1\t98 44\n",
      "28\t241\t0.0\t1\t178 63\n",
      "29\t148\t0.0\t1\t123 25\n",
      "30\t527\t0.0\t1\t439 88\n",
      "31\t101\t0.0\t1\t74 27\n",
      "32\t38\t0.0\t1\t30 8\n",
      "33\t53\t0.0\t1\t45 8\n",
      "34\t29\t0.0\t1\t19 10\n",
      "35\t30\t0.0\t1\t25 5\n",
      "36\t65\t0.0\t1\t53 12\n",
      "37\t44\t0.0\t1\t38 6\n",
      "38\t8\t0.0\t1\t6 2\n",
      "39\t6\t0.0\t1\t6\n",
      "40\t18\t0.0\t1\t14 4\n",
      "41\t13\t0.0\t1\t11 2\n",
      "42\t80\t0.0\t1\t65 15\n",
      "43\t21\t0.0\t1\t20 1\n",
      "44\t14\t0.0\t1\t12 2\n",
      "45\t21\t0.0\t1\t17 4\n",
      "46\t33\t0.0\t1\t25 8\n",
      "47\t49\t0.0\t1\t43 6\n",
      "48\t31\t0.0\t1\t27 4\n",
      "49\t5\t0.0\t1\t4 1\n",
      "50\t16\t0.0\t1\t11 5\n",
      "51\t8\t0.0\t1\t6 2\n",
      "52\t18\t0.0\t1\t13 5\n",
      "53\t4\t0.0\t1\t4\n",
      "54\t3\t0.0\t1\t3\n",
      "55\t4\t0.0\t1\t4\n",
      "56\t7\t0.0\t1\t3 4\n",
      "57\t6\t0.0\t1\t6\n",
      "58\t2\t0.0\t1\t0 2\n",
      "59\t2\t0.0\t1\t0 2\n",
      "60\t2\t0.0\t1\t1 1\n",
      "61\t59\t0.0\t1\t54 5\n",
      "64\t1\t0.0\t1\t0 1\n",
      "65\t1\t0.0\t1\t1\n",
      "66\t6\t0.0\t1\t5 1\n",
      "70\t2\t0.0\t1\t1 1\n",
      "71\t4\t0.0\t1\t3 1\n",
      "72\t1\t0.0\t1\t1\n",
      "76\t2\t0.0\t1\t2\n",
      "77\t9\t0.0\t1\t8 1\n",
      "78\t6\t0.0\t1\t6\n",
      "84\t5\t0.0\t1\t5\n",
      "85\t16\t0.0\t1\t14 2\n",
      "86\t4\t0.0\t1\t4\n",
      "87\t22\t0.0\t1\t18 4\n",
      "95\t2\t0.0\t1\t0 2\n",
      "96\t1\t0.0\t1\t1\n",
      "99\t6\t0.0\t1\t5 1\n",
      "100\t1\t0.0\t1\t1\n",
      "105\t4\t0.0\t1\t4\n",
      "106\t5\t0.0\t1\t4 1\n",
      "112\t8\t0.0\t1\t6 2\n",
      "113\t1\t0.0\t1\t1\n",
      "114\t8\t0.0\t1\t6 2\n",
      "115\t1\t0.0\t1\t0 1\n",
      "116\t1\t0.0\t1\t0 1\n",
      "121\t10\t0.0\t1\t9 1\n",
      "122\t3\t0.0\t1\t2 1\n",
      "123\t1\t0.0\t1\t1\n",
      "127\t1\t0.0\t1\t0 1\n",
      "131\t2\t0.0\t1\t2\n",
      "135\t2\t0.0\t1\t1 1\n",
      "137\t1\t0.0\t1\t0 1\n",
      "140\t1\t0.0\t1\t0 1\n",
      "142\t1\t0.0\t1\t1\n",
      "144\t15\t0.0\t1\t13 2\n",
      "146\t3\t0.0\t1\t3\n",
      "150\t2\t0.0\t1\t1 1\n",
      "157\t9\t0.0\t1\t9\n",
      "158\t2\t0.0\t1\t1 1\n",
      "159\t1\t0.0\t1\t0 1\n",
      "160\t1\t0.0\t1\t1\n",
      "161\t8\t0.0\t1\t8\n",
      "163\t3\t0.0\t1\t3\n",
      "164\t1\t0.0\t1\t0 1\n",
      "170\t2\t0.0\t1\t2\n",
      "171\t1\t0.0\t1\t0 1\n",
      "173\t1\t0.0\t1\t0 1\n",
      "174\t1\t0.0\t1\t0 1\n",
      "177\t11\t0.0\t1\t10 1\n",
      "181\t1\t0.0\t1\t0 1\n",
      "186\t2\t0.0\t1\t0 2\n",
      "191\t1\t0.0\t1\t0 1\n",
      "192\t1\t0.0\t1\t0 1\n",
      "197\t1\t0.0\t1\t1\n",
      "205\t3\t0.0\t1\t0 3\n",
      "209\t3\t0.0\t1\t1 2\n",
      "212\t1\t0.0\t1\t1\n",
      "214\t1\t0.0\t1\t1\n",
      "220\t10\t0.0\t1\t7 3\n",
      "222\t3\t0.0\t1\t0 3\n",
      "224\t1\t0.0\t1\t1\n",
      "226\t1\t0.0\t1\t0 1\n",
      "232\t4\t0.0\t1\t4\n",
      "233\t1\t0.0\t1\t1\n",
      "234\t2\t0.0\t1\t1 1\n",
      "239\t2\t0.0\t1\t2\n",
      "241\t8\t0.0\t1\t7 1\n",
      "246\t3\t0.0\t1\t3\n",
      "247\t2\t0.0\t1\t1 1\n",
      "250\t1\t0.0\t1\t0 1\n",
      "251\t14\t0.0\t1\t0 14\n"
     ]
    }
   ],
   "source": [
    "for library in libraries:\n",
    "    raw = library[2]\n",
    "    read1 = library[3]\n",
    "    read2 = library[4]\n",
    "    \n",
    "    !cutadapt -a ATTAGAWACCCBDGTAGTCC -o $raw/modified/read1_noprimer.fastq $raw/modified/read1_mod.fastq\n",
    "    !cutadapt -a TTACCGCGGCKGCTGGCAC -o $raw/modified/read2_noprimer.fastq $raw/modified/read2_mod.fastq\n",
    "    \n",
    "    # Delete unneeded intermediate files\n",
    "    !rm $raw/modified/read*_mod.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Filter short reads\n",
    "\n",
    "Remove sequences with from primer-removed data with less than 100 bp from all files (too short).\n",
    "\n",
    "SLOW STEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    raw = library[2]\n",
    "    os.system(' '.join([\n",
    "        \"python /home/cassi/scripts/qiime2_filter_shortreads.py\",\n",
    "        raw+\"/modified/\"\n",
    "    ]))\n",
    "    \n",
    "    # Remove unneeded intermediate files\n",
    "    !rm $raw/modified/*_noprimer.fastq\n",
    "    !rm $raw/modified/index*_mod.fastq\n",
    "    \n",
    "    # Recompress modified read files\n",
    "    !pigz $raw/modified/read*_filtered.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Concatenate barcodes\n",
    "\n",
    "This step calls a custom script, \"concatenate_barcodes_qiime2.py\". The script must be shuttled to the command line instead of run directly in jupyter notebooks because jupyter has memory issues that truncates the barcodes file without an error (I think).\n",
    "\n",
    "This script requires your index1 and index2 files to be named index1_mod.fastq and index2_mod.fastq and be located in a directory within the raw read directory called 'modified'. This should have been taken care of in earlier steps in this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    raw = library[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"python /home/cassi/scripts/qiime2_concatenate_barcodes.py\",\n",
    "        raw+\"/modified\"]))\n",
    "  \n",
    "    # Recompress modified index files and newly created barcodes.fastq file\n",
    "    !pigz $raw/modified/*.fastq   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Move raw data to library directories\n",
    "\n",
    "Creates intermediate directory in library directory. All subsequent files except for the final files will be placed there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    proc = library[1]\n",
    "    raw = library[2]\n",
    "    \n",
    "    # Create output directory if it doesn't exist already\n",
    "    if not os.path.isdir(os.path.join(proc, \"intermediate\")):\n",
    "        !mkdir $proc/intermediate\n",
    "    \n",
    "    # Create a symbolic link to the read data\n",
    "    # QIIME2 import requires a directory containing files named: forward.fastq.gz, reverse.fastq.gz and barcodes.fastq.gz \n",
    "    !ln -s $raw/modified/read1_filtered.fastq.gz $proc/intermediate/forward.fastq.gz\n",
    "    !ln -s $raw/modified/read2_filtered.fastq.gz $proc/intermediate/reverse.fastq.gz\n",
    "    \n",
    "    # Move concatenated barcodes to project directory\n",
    "    !cp $raw/modified/barcodes.fastq.gz $proc/intermediate/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Import into QIIME2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime tools import\",\n",
    "        \"--type EMPPairedEndSequences\",\n",
    "        \"--input-path \"+proc+\"/intermediate\",\n",
    "        \"--output-path \"+proc+\"/intermediate/\"+name+\".qza\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Demultiplex\n",
    "\n",
    "The barcode you supply to QIIME is now a concatenation of your forward and reverse barcode. Your 'forward' barcode is actually the reverse complement of your reverse barcode and the 'reverse' is your forward barcode.\n",
    "\n",
    "Slow step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    metadata = library[7]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux emp-paired\",\n",
    "        \"--m-barcodes-file \"+proc+\"/\"+metadata,\n",
    "        \"--m-barcodes-column BarcodeSequence\",\n",
    "        \"--p-no-golay-error-correction\",\n",
    "        \"--i-seqs \"+proc+\"/intermediate/\"+name+\".qza\",\n",
    "        \"--o-per-sample-sequences \"+proc+\"/intermediate/\"+name+\".demux\",\n",
    "        \"--o-error-correction-details \"+proc+\"/intermediate/\"+name+\".demux-details.qza\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Visualize quality scores\n",
    "\n",
    "Drop output from below command into https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux summarize\",\n",
    "        \"--i-data \"+proc+\"/intermediate/\"+name+\".demux.qza\",\n",
    "        \"--o-visualization \"+proc+\"/intermediate/\"+name+\".demux.qzv\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Trimming parameters | USER INPUT REQUIRED\n",
    "\n",
    "Based on the quality scores of the bp along the reads, choose trim and truncate values for the forward and reverse reads. Trim refers to the start of a sequence and truncate the total length (i.e. number of bases to remove from end).\n",
    "\n",
    "All trimming parameters must be the same for datasets that will be directly compared to one-another because ASVs are determined, in part, by sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input your trimming parameters into a python dictionary for all libraries\n",
    "# trim_dict = {\"LibraryName1\":[trim_forward, truncate_forward, trim_reverse, truncate_reverse],\n",
    "#              \"LibraryName2\":[trim_forward, truncate_forward, trim_reverse, truncate_reverse],\n",
    "#               etc...}\n",
    "\n",
    "# The example in the Atacama Desert Tutorial trims 13 bp from the start of each read and does not remove any bases from the end of the 150 bp reads:\n",
    "#  --p-trim-left-f 13 \\  \n",
    "#  --p-trim-left-r 13 \\\n",
    "#  --p-trunc-len-f 150 \\\n",
    "#  --p-trunc-len-r 150\n",
    "\n",
    "trim_dict = {\"NIFA_micro\":[15,220,15,220]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Trim, denoise and join (aka 'merge') reads using DADA2\n",
    "\n",
    "See the [QIIME2 dada2 denoise-paired documentation](https://docs.qiime2.org/2021.8/plugins/available/dada2/denoise-paired/) for the default parameters used.\n",
    "\n",
    "Slow step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime dada2 denoise-paired\",\n",
    "        \"--i-demultiplexed-seqs \"+proc+\"/intermediate/\"+name+\".demux.qza\",\n",
    "        \"--o-table \"+proc+\"/intermediate/\"+name+\".table.qza\",\n",
    "        \"--o-representative-sequences \"+proc+\"/intermediate/\"+name+\".rep-seqs.qza\",\n",
    "        \"--o-denoising-stats \"+proc+\"/intermediate/\"+name+\".denoising-stats.qza\",\n",
    "        \"--p-trim-left-f \"+str(trim_dict[name][0]),\n",
    "        \"--p-trim-left-r \"+str(trim_dict[name][2]),\n",
    "        \"--p-trunc-len-f \"+str(trim_dict[name][1]),\n",
    "        \"--p-trunc-len-r \"+str(trim_dict[name][3]),\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Create summary of ASVs\n",
    "\n",
    "Drop outputs from below command into https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    metadata = library[7]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table summarize\",\n",
    "        \"--i-table \"+proc+\"/intermediate/\"+name+\".table.qza\",\n",
    "        \"--o-visualization \"+proc+\"/intermediate/\"+name+\".table.qzv\",\n",
    "        \"--m-sample-metadata-file \"+proc+\"/\"+metadata\n",
    "    ]))\n",
    "\n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table tabulate-seqs\",\n",
    "        \"--i-data \"+proc+\"/intermediate/\"+name+\".rep-seqs.qza\",\n",
    "        \"--o-visualization \"+proc+\"/intermediate/\"+name+\".rep-seqs.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13: Classify sequences\n",
    "\n",
    "Different QIIME2 versions can conflict with certain classifier database versions. This section will likely need to be updated. Download the latest classifiers here: https://docs.qiime2.org/2021.4/data-resources/\n",
    "\n",
    "* Using SILVA v138 pre-built classifier trained on scikit learn 0.24.1.\n",
    "\n",
    "View output in https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use classifier for chosen database\n",
    "try:\n",
    "    if db == \"GreenGenes\":\n",
    "        classifier_db = \"/home/db/GreenGenes/qiime2_13.8.99_515.806_nb.classifier.qza\" # out of date\n",
    "    else:\n",
    "        classifier_db = \"~/databases/silva/silva-138-99-515-806-nb-classifier.qza\"\n",
    "except:\n",
    "        classifier_db = \"~/databases/silva/silva-138-99-515-806-nb-classifier.qza\"\n",
    "        \n",
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    metadata = library[7]\n",
    "    domain = library[8]\n",
    "\n",
    "    # Classify\n",
    "    if domain == 'bacteria':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier\",\n",
    "            classifier_db,\n",
    "            \"--i-reads \"+proc+\"/intermediate/\"+name+\".rep-seqs.qza\",\n",
    "            \"--o-classification \"+proc+\"/intermediate/\"+name+\".taxonomy.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "    if domain == 'fungi':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier /home/db/UNITE/qiime2_unite_ver7.99_20.11.2016_classifier.qza\", # out of date\n",
    "            \"--i-reads \"+proc+\"/intermediate/\"+name+\".rep-seqs.qza\",\n",
    "            \"--o-classification \"+proc+\"/intermediate/\"+name+\".taxonomy.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "    # Output summary\n",
    "    os.system(' '.join([\n",
    "        \"qiime metadata tabulate\",\n",
    "        \"--m-input-file \"+proc+\"/intermediate/\"+name+\".taxonomy.qza\",\n",
    "        \"--o-visualization \"+proc+\"/intermediate/\"+name+\".taxonomy-summary.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 14: Combine representative sequences for tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bacteria\n",
    "\n",
    "# Create representative sequences file\n",
    "repseqsbac = []\n",
    "\n",
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    domain = library[8]\n",
    "    \n",
    "    # Create list of rep seq files\n",
    "    if domain == \"bacteria\":\n",
    "        repseqsbac.append(\"--i-data \" + os.path.join(proc, \"intermediate\", name+\".rep-seqs.qza\"))\n",
    "\n",
    "# Create tree directory\n",
    "if not os.path.isdir(os.path.join(project, \"tree\")):\n",
    "    !mkdir $project/tree\n",
    "    \n",
    "# Merge rep sequences from all bacterial libraries for tree\n",
    "os.system(' '.join([\n",
    "    \"qiime feature-table merge-seqs\",\n",
    "    \" \".join(repseqsbac),\n",
    "    \"--o-merged-data \"+project+\"/tree/\"+treename+\".rep-seqs-merged.qza\"\n",
    "]))\n",
    "\n",
    "# Fungi\n",
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    domain = library[8]\n",
    "    \n",
    "    # Create representative sequences file\n",
    "    repseqsfung = []\n",
    "    \n",
    "    if domain == \"fungi\":\n",
    "        repseqsfung.append(\"--i-data \" + os.path.join(proc, \"intermediate\", name+\".rep-seqs.qza\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 15: Make phylogenetic tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    domain = library[8]\n",
    "\n",
    "    if domain == \"bacteria\":\n",
    "        # Generate alignment with MAFFT\n",
    "        os.system(' '.join([\n",
    "            \"qiime alignment mafft\",\n",
    "            \"--i-sequences \"+project+\"/tree/\"+treename+\".rep-seqs-merged.qza\",\n",
    "            \"--o-alignment \"+project+\"/tree/\"+treename+\".rep-seqs-merged-aligned.qza\",\n",
    "            \"--p-n-threads\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "        # Mask hypervariable regions in alignment\n",
    "        os.system(' '.join([\n",
    "            \"qiime alignment mask\",\n",
    "            \"--i-alignment \"+project+\"/tree/\"+treename+\".rep-seqs-merged-aligned.qza\",\n",
    "            \"--o-masked-alignment \"+project+\"/tree/\"+treename+\".rep-seqs-merged-aligned-masked.qza\",\n",
    "        ]))\n",
    "\n",
    "        # Generate tree with FastTree\n",
    "        os.system(' '.join([\n",
    "            \"qiime phylogeny fasttree\",\n",
    "            \"--i-alignment \"+project+\"/tree/\"+treename+\".rep-seqs-merged-aligned-masked.qza\",\n",
    "            \"--o-tree \"+project+\"/tree/\"+treename+\".tree-unrooted.qza\",\n",
    "            \"--p-n-threads\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "        # Root the tree\n",
    "        os.system(' '.join([\n",
    "            \"qiime phylogeny midpoint-root\",\n",
    "            \"--i-tree \"+project+\"/tree/\"+treename+\".tree-unrooted.qza\",\n",
    "            \"--o-rooted-tree \"+project+\"/tree/\"+treename+\".tree-rooted.qza\"\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 16: Reformat taxonomy\n",
    "\n",
    "Define function to tidy the taxonomy and make it compatible with phyloseq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_taxonomy(tax_dirty, min_support):\n",
    "    output = open(re.sub(\".tsv\",\"-fixed.tsv\",tax_dirty), \"w\")\n",
    "    \n",
    "    full_rank_length = 7\n",
    "    output.write(\"\\t\".join([\"ASV\",\"Domain\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\",\"Species\"])+\"\\n\")\n",
    "\n",
    "    with open(tax_dirty, \"r\") as f:\n",
    "        next(f)\n",
    "\n",
    "        for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "\n",
    "                read_id = line[0]\n",
    "                tax_string = line[1]\n",
    "                \n",
    "                ## Remove taxonomy prefixes and underscores (only coded for Silva classifications so far)\n",
    "                if db == \"Silva\":\n",
    "                    tax_string = re.sub(\"[a-z]__\", \"\", tax_string)\n",
    "                    tax_string = re.sub(\"_\", \" \", tax_string)\n",
    "\n",
    "                # Split full rank into ranks\n",
    "                full_rank = tax_string.split(\";\")\n",
    "\n",
    "                ## Identify the lowest classified taxonomic rank\n",
    "                # Account for cases when a taxonomic rank contains an empty space (common in GreenGenes output)\n",
    "                last_classified = full_rank[len(full_rank)-1]            \n",
    "\n",
    "                count = 1\n",
    "                while last_classified == \" \":\n",
    "                    last_classified = full_rank[len(full_rank)-count]\n",
    "                    count = count + 1\n",
    "\n",
    "                # Annotate the last classified as 'putative' if it does not meet the minimum support criteria\n",
    "                if float(line[2]) < float(min_support):\n",
    "                        full_rank[full_rank.index(last_classified)] = \"putative \"+last_classified\n",
    "                        last_classified = \"putative \"+last_classified\n",
    "\n",
    "                # Add in columns containing unclassified taxonomic information\n",
    "                for n in range(full_rank.index(last_classified)+1, full_rank_length, 1):               \n",
    "                    try:\n",
    "                        full_rank[n] = \"unclassified \"+last_classified\n",
    "                    except:\n",
    "                        full_rank.append(\"unclassified \"+last_classified)\n",
    "\n",
    "                # Write taxonomy to file\n",
    "                output.write(read_id+\"\\t\"+'\\t'.join(full_rank)+\"\\n\")\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 17: Export from QIIME2\n",
    "\n",
    "All final files will be placed in 'final' directory in library directories. Final tree will be in a 'tree' directory in the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    metadata = library[7]\n",
    "\n",
    "    # Final output paths\n",
    "    fasta_final = proc+\"/final/\"+name+\".rep-seqs-final.fasta\"\n",
    "    tax_final= proc+\"/final/\"+name+\".taxonomy-final.tsv\"\n",
    "    count_final = proc+\"/final/\"+name+\".counts-final.biom\"\n",
    "    \n",
    "    # Make final data directories\n",
    "    if not os.path.isdir(os.path.join(proc, \"final\")):\n",
    "        !mkdir $proc/final\n",
    "        \n",
    "    # Export ASV table\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+proc+\"/intermediate/\"+name+\".table.qza\",\n",
    "        \"--output-path \"+proc+\"/intermediate/\"\n",
    "    ]))\n",
    "\n",
    "    # Export taxonomic classifications\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+proc+\"/intermediate/\"+name+\".taxonomy.qza\",\n",
    "        \"--output-path \"+proc+\"/intermediate/\"\n",
    "    ]))\n",
    "    \n",
    "    # Reformat classifications  \n",
    "    format_taxonomy(proc+\"/intermediate/taxonomy.tsv\", min_support)\n",
    "\n",
    "    # Export representative sequences\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+proc+\"/intermediate/\"+name+\".rep-seqs.qza\",\n",
    "        \"--output-path \"+proc+\"/intermediate/\"\n",
    "    ]))\n",
    "    \n",
    "    # Rename exported files and move to final directory\n",
    "    !mv $proc/intermediate/dna-sequences.fasta $fasta_final\n",
    "    !mv $proc/intermediate/feature-table.biom $count_final\n",
    "    !mv $proc/intermediate/taxonomy-fixed.tsv $tax_final\n",
    "    \n",
    "    # Reformat count table\n",
    "    tmp_tsv = re.sub(name+\".counts-final.biom\", \"tmp.tsv\", count_final)\n",
    "    tmp2_tsv = re.sub(name+\".counts-final.biom\", \"tmp2.tsv\", count_final)\n",
    "    count_tsv = re.sub(\".biom\", \".tsv\", count_final)\n",
    "    !biom convert -i $count_final -o $tmp_tsv --to-tsv # conver to .tsv\n",
    "    !tail -n +2 $tmp_tsv > $tmp2_tsv # remove header\n",
    "    !sed 's/\\#OTU ID/ASV/g' $tmp2_tsv > $count_tsv # replace OTU with ASV\n",
    "    !rm $tmp_tsv\n",
    "    !rm $tmp2_tsv\n",
    "    \n",
    "# Export tree\n",
    "os.system(' '.join([\n",
    "    \"qiime tools export\",\n",
    "    \"--input-path \"+project+\"/tree/\"+treename+\".tree-rooted.qza\",\n",
    "    \"--output-path \"+project+\"/tree/\"\n",
    "]))\n",
    "\n",
    "# Rename tree file\n",
    "tree_final = project+\"/tree/\"+treename+\".tree-final.nwk\"\n",
    "!mv $project/tree/\"tree.nwk\" $tree_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export tree\n",
    "os.system(' '.join([\n",
    "    \"qiime tools export\",\n",
    "    \"--input-path \"+project+\"/tree/\"+treename+\".tree-rooted.qza\",\n",
    "    \"--output-path \"+project+\"/tree/\"\n",
    "]))\n",
    "\n",
    "# Rename tree file\n",
    "tree_final = project+\"/tree/\"+treename+\".tree-final.nwk\"\n",
    "!mv $project/tree/\"tree.nwk\" $tree_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qiime tools export --input-path /home/cassi/NIFA/data_amplicon/NIFA_micro/tree/NIFA.rep-seqs-merged.qza --output-path /home/cassi/NIFA/data_amplicon/NIFA_micro/tree/\n"
     ]
    }
   ],
   "source": [
    "# Export merged rep seqs\n",
    "os.print(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+project+\"/tree/\"+treename+\".rep-seqs-merged.qza\",\n",
    "        \"--output-path \"+project+\"/tree/\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
