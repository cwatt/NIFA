{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: NIFA\n",
    "\n",
    "Cassandra Wattenburger, 10/17/22\n",
    "\n",
    "### Notes:\n",
    "* QIIME2 v2021.4\n",
    "* Micro run to test internal standard concentrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline information\n",
    "\n",
    "### Pipeline to process raw sequences with DADA2 ###\n",
    "* Prep for import into QIIME2 (modify sequence IDs and combine two index files)\n",
    "* Import into QIIME2\n",
    "* Demultiplex\n",
    "* Denoise and merge with DADA2\n",
    "* Prepare ASV tables and representative sequences *(Note: sample names starting with a digit will break this step)*\n",
    "* Classify sequences\n",
    "* Construct phylogenetic tree\n",
    "* Export from QIIME2\n",
    "\n",
    "*100% Appropriated from the \"Atacama Desert Tutorial\" for QIIME2*\n",
    "\n",
    "### Pipeline can handle both 16S rRNA gene and ITS sequences ###\n",
    "* Tested on 515f and 806r\n",
    "* Tested on ITS1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start\n",
    "\n",
    "### Commands to install dependencies ####\n",
    "##### || QIIME2 and biopython ||\n",
    "QIIME2 is still actively in development with frequent new releases. Check for the most up-to-date version and use that.\n",
    "\n",
    "Install QIIME2: \n",
    "* <https://docs.qiime2.org/2017.11/install/native/#install-qiime-2-within-a-conda-environment>, follow the instructions to install QIIME2 in Linux (64-bit).\n",
    "\n",
    "Activate the QIIME2 environment:\n",
    "* source activate [qiime2-pipeline-name]\n",
    "\n",
    "After you install and activate the QIIME2 environment, you must also install biopython for the barcode concatenation step to work. To install biopython make sure the QIIME2 environment is activated and run:\n",
    "* conda install -c anaconda biopython\n",
    "\n",
    "Install cutadapt to the QIIME2 environment as well. Cutadapt removes primers from the sequences.\n",
    "* conda install -c bioconda cutadapt\n",
    "\n",
    "##### || Copyrighter rrn database ||\n",
    "The script will automatically install the curated GreenGenes rrn attribute database: https://github.com/fangly/AmpliCopyrighter\n",
    "\n",
    "#### Citations\n",
    "* Caporaso, J. G., Kuczynski, J., Stombaugh, J., Bittinger, K., Bushman, F. D., Costello, E. K., *et al.* (2010). QIIME allows analysis of high-throughput community sequencing data. Nature methods, 7(5), 335-336.\n",
    "\n",
    "* Angly, F. E., Dennis, P. G., Skarshewski, A., Vanwonterghem, I., Hugenholtz, P., & Tyson, G. W. (2014). CopyRighter: a rapid tool for improving the accuracy of microbial community profiles through lineage-specific gene copy number correction. Microbiome, 2(1), 11.\n",
    "\n",
    "### Using jupyter notebook screens ###\n",
    "\n",
    "With the QIIME2 environment activated, open your jupyter notebook screen in the directory containing this script:\n",
    "* jupyter-n [screen-name] [port #]\n",
    "\n",
    "See [these instructions](https://github.com/buckleylab/Buckley_lab_protocols/blob/master/Using_the_server/getting_started_on_server.md#make-jupyter-notebook-screens-command) for how to set up and use this command on the server.\n",
    "\n",
    "### Directory and data organization ###\n",
    "\n",
    "This pipeline assumes that you've organized your data in a certain way:\n",
    "* Each library of raw data is contained in a separate directory\n",
    "* Each library has a separate working directory within a larger project directory\n",
    "* Tree construction assumes all 16S libraries processed together will be analyzed together, so one tree is made based on all 16S libraries and is placed in a separate tree directory within the project directory\n",
    "\n",
    "### Troubleshooting tip ###\n",
    "Replace os.system with print, and copy/paste the output into the command line to view the error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: User Input\n",
    "\n",
    "Metadata requirements:\n",
    "* Must be located in each library directory\n",
    "* Must be .tsv format \n",
    "* First column is named \"SampleID\" with sample names as rows\n",
    "* Contains another column named \"BarcodeSequence\" with the relevant barcode seqeunces as rows (rev. comp. reverse barcode sequence concatenated with forward barcode sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np\n",
    "\n",
    "# Prepare an object with the name of the library and all related file paths\n",
    "# datasets = [['library name prefix', 'processed data directory path', 'raw data directory', 'read1 file name', 'read2 file name', 'index1 file name', 'index2 file name', 'metadata file name', 'domain of life (bacteria or fungi)'], ...]\n",
    "project = \"/home/cassi/NIFA/data_amplicon/NIFA_micro2\" # this can be the same as the library directory if you only have one library to process\n",
    "libraries = [['NIFA_micro2', \n",
    "             '/home/cassi/NIFA/data_amplicon/NIFA_micro2', \n",
    "             '/home/backup_files/raw_reads/AM.CW.EW.2022',\n",
    "             '179363_GFCH9_Ryan_Cassi_Angie_Lib_11-30_S1_R1_001.fastq.gz', \n",
    "             '179363_GFCH9_Ryan_Cassi_Angie_Lib_11-30_S1_R2_001.fastq.gz', \n",
    "             '179363_GFCH9_Ryan_Cassi_Angie_Lib_11-30_S1_I1_001.fastq.gz', \n",
    "             '179363_GFCH9_Ryan_Cassi_Angie_Lib_11-30_S1_I2_001.fastq.gz', \n",
    "             'NIFA_micro2_demultiplex.tsv', # I just dumped all of the plate 3 primers in, because I forgot which ones I used and don't have my lab notebook handy\n",
    "             'bacteria']]\n",
    "\n",
    "# Set # of processors\n",
    "processors = 10\n",
    "\n",
    "# Which bacterial database will you use? Silva or GreenGenes\n",
    "db = \"Silva\"\n",
    "\n",
    "# Phylogenetic tree (non-fungal data)\n",
    "treename = \"NIFA\" # name prefix for the tree file\n",
    "\n",
    "## Enter minimum support for keeping QIIME classification\n",
    "# Note: Classifications that do not meet this criteria will be retained, labeled 'putative'\n",
    "min_support = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Truncate sequence identifiers\n",
    "\n",
    "This step removes a portion at the end of the sequence ID that is incompatible with QIIME2. It will also create a modified directory in your raw data directory to house the modified data. The original raw data will not be modified, only the copies.\n",
    "\n",
    "Slow step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    raw = library[2]\n",
    "    read1 = library[3]\n",
    "    read2 = library[4]\n",
    "    index1 = library[5]\n",
    "    index2 = library[6]\n",
    "    \n",
    "    # Create a directory to place modified sequence data\n",
    "    if not os.path.isdir(os.path.join(raw, \"modified\")):\n",
    "        !mkdir $raw/modified\n",
    "    \n",
    "    # Copy/paste all raw sequence data into modified directory\n",
    "    !cp $raw/*.fastq.gz $raw/modified\n",
    "    \n",
    "    # Decompress all files\n",
    "    !unpigz $raw/modified/*.fastq.gz\n",
    "    \n",
    "    # Decompressed file names\n",
    "    read1decomp = re.sub(\".fastq.gz\", \".fastq\", read1)\n",
    "    read2decomp = re.sub(\".fastq.gz\", \".fastq\", read2)\n",
    "    index1decomp = re.sub(\".fastq.gz\", \".fastq\", index1)\n",
    "    index2decomp = re.sub(\".fastq.gz\", \".fastq\", index2)\n",
    "    \n",
    "    # Remove problematic part of sequence IDs\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$read1decomp > $raw/modified/read1_mod.fastq\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$read2decomp > $raw/modified/read2_mod.fastq\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$index1decomp > $raw/modified/index1_mod.fastq\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$index2decomp > $raw/modified/index2_mod.fastq\n",
    "    \n",
    "    # Delete file copies\n",
    "    !rm $raw/modified/$read1decomp\n",
    "    !rm $raw/modified/$read2decomp\n",
    "    !rm $raw/modified/$index1decomp\n",
    "    !rm $raw/modified/$index2decomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Remove primer from sequences\n",
    "\n",
    "There may still be portions of the primers left in the read sequences that need to be removed. Use cutadapt to remove those portions. Read1 will have the reverse complement of the reverse primer and read2 will have the reverse complement of the forward primer in some sequences on the 3' end.\n",
    "\n",
    "You will get a warning that the adapter is preceded by \"A\" or \"G\" extremely often. These are the link sequences in the primer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is cutadapt 3.4 with Python 3.8.8\n",
      "Command line parameters: -a ATTAGAWACCCBDGTAGTCC -o /home/backup_files/raw_reads/AM.CW.EW.2022/modified/read1_noprimer.fastq /home/backup_files/raw_reads/AM.CW.EW.2022/modified/read1_mod.fastq\n",
      "Processing reads on 1 core in single-end mode ...\n",
      "[   8=-------] 00:00:56     5,670,518 reads  @      9.9 µs/read;   6.06 M reads/minute\n",
      "Finished in 56.11 s (10 µs/read; 6.06 M reads/minute).\n",
      "\n",
      "=== Summary ===\n",
      "\n",
      "Total reads processed:               5,670,518\n",
      "Reads with adapters:                    14,273 (0.3%)\n",
      "Reads written (passing filters):     5,670,518 (100.0%)\n",
      "\n",
      "Total basepairs processed: 1,423,300,018 bp\n",
      "Total written (filtered):  1,423,113,825 bp (100.0%)\n",
      "\n",
      "=== Adapter 1 ===\n",
      "\n",
      "Sequence: ATTAGAWACCCBDGTAGTCC; Type: regular 3'; Length: 20; Trimmed: 14273 times\n",
      "\n",
      "No. of allowed errors:\n",
      "1-9 bp: 0; 10-19 bp: 1; 20 bp: 2\n",
      "\n",
      "Bases preceding removed adapters:\n",
      "  A: 18.3%\n",
      "  C: 22.7%\n",
      "  G: 40.0%\n",
      "  T: 19.0%\n",
      "  none/other: 0.0%\n",
      "\n",
      "Overview of removed sequences\n",
      "length\tcount\texpect\tmax.err\terror counts\n",
      "3\t8502\t88601.8\t0\t8502\n",
      "4\t2253\t22150.5\t0\t2253\n",
      "5\t491\t5537.6\t0\t491\n",
      "6\t276\t1384.4\t0\t276\n",
      "7\t79\t346.1\t0\t79\n",
      "8\t109\t86.5\t0\t109\n",
      "9\t63\t21.6\t0\t27 36\n",
      "10\t125\t5.4\t1\t107 18\n",
      "11\t69\t1.4\t1\t53 16\n",
      "12\t21\t0.3\t1\t17 4\n",
      "13\t43\t0.1\t1\t33 10\n",
      "14\t16\t0.0\t1\t13 3\n",
      "15\t13\t0.0\t1\t11 2\n",
      "16\t9\t0.0\t1\t7 2\n",
      "17\t20\t0.0\t1\t17 3\n",
      "18\t57\t0.0\t1\t38 15 4\n",
      "19\t11\t0.0\t1\t9 2\n",
      "20\t14\t0.0\t2\t10 1 3\n",
      "21\t15\t0.0\t2\t12 1 2\n",
      "22\t8\t0.0\t2\t6 1 1\n",
      "23\t35\t0.0\t2\t25 7 3\n",
      "24\t30\t0.0\t2\t22 4 4\n",
      "25\t11\t0.0\t2\t8 3\n",
      "26\t22\t0.0\t2\t19 2 1\n",
      "27\t124\t0.0\t2\t108 9 7\n",
      "28\t123\t0.0\t2\t104 10 9\n",
      "29\t382\t0.0\t2\t315 39 28\n",
      "30\t349\t0.0\t2\t291 43 15\n",
      "31\t26\t0.0\t2\t19 3 4\n",
      "32\t7\t0.0\t2\t4 3\n",
      "33\t28\t0.0\t2\t19 5 4\n",
      "34\t12\t0.0\t2\t8 2 2\n",
      "35\t14\t0.0\t2\t13 0 1\n",
      "36\t51\t0.0\t2\t41 5 5\n",
      "37\t3\t0.0\t2\t2 1\n",
      "38\t11\t0.0\t2\t8 1 2\n",
      "39\t5\t0.0\t2\t2 3\n",
      "40\t1\t0.0\t2\t1\n",
      "41\t15\t0.0\t2\t12 1 2\n",
      "42\t201\t0.0\t2\t162 22 17\n",
      "43\t2\t0.0\t2\t2\n",
      "44\t13\t0.0\t2\t11 2\n",
      "45\t43\t0.0\t2\t33 8 2\n",
      "46\t6\t0.0\t2\t6\n",
      "47\t21\t0.0\t2\t6 1 14\n",
      "48\t10\t0.0\t2\t9 0 1\n",
      "49\t10\t0.0\t2\t8 1 1\n",
      "50\t3\t0.0\t2\t3\n",
      "51\t4\t0.0\t2\t3 1\n",
      "52\t5\t0.0\t2\t4 1\n",
      "53\t2\t0.0\t2\t2\n",
      "54\t1\t0.0\t2\t1\n",
      "55\t1\t0.0\t2\t1\n",
      "57\t2\t0.0\t2\t2\n",
      "58\t1\t0.0\t2\t1\n",
      "59\t12\t0.0\t2\t11 1\n",
      "60\t2\t0.0\t2\t1 0 1\n",
      "61\t2\t0.0\t2\t2\n",
      "62\t4\t0.0\t2\t3 0 1\n",
      "64\t2\t0.0\t2\t2\n",
      "65\t2\t0.0\t2\t0 1 1\n",
      "71\t3\t0.0\t2\t3\n",
      "72\t4\t0.0\t2\t0 1 3\n",
      "76\t2\t0.0\t2\t2\n",
      "77\t3\t0.0\t2\t1 2\n",
      "80\t4\t0.0\t2\t2 1 1\n",
      "81\t4\t0.0\t2\t2 0 2\n",
      "82\t2\t0.0\t2\t1 1\n",
      "83\t1\t0.0\t2\t0 0 1\n",
      "84\t5\t0.0\t2\t3 2\n",
      "85\t11\t0.0\t2\t10 1\n",
      "86\t9\t0.0\t2\t8 1\n",
      "87\t9\t0.0\t2\t9\n",
      "90\t2\t0.0\t2\t2\n",
      "91\t2\t0.0\t2\t1 1\n",
      "92\t3\t0.0\t2\t3\n",
      "96\t2\t0.0\t2\t0 0 2\n",
      "99\t1\t0.0\t2\t1\n",
      "100\t7\t0.0\t2\t3 3 1\n",
      "105\t1\t0.0\t2\t1\n",
      "106\t3\t0.0\t2\t3\n",
      "107\t2\t0.0\t2\t1 0 1\n",
      "108\t1\t0.0\t2\t1\n",
      "110\t2\t0.0\t2\t1 1\n",
      "112\t2\t0.0\t2\t2\n",
      "117\t1\t0.0\t2\t1\n",
      "118\t2\t0.0\t2\t2\n",
      "119\t1\t0.0\t2\t0 0 1\n",
      "120\t3\t0.0\t2\t1 1 1\n",
      "121\t32\t0.0\t2\t22 2 8\n",
      "122\t1\t0.0\t2\t1\n",
      "124\t3\t0.0\t2\t2 1\n",
      "127\t2\t0.0\t2\t0 0 2\n",
      "131\t5\t0.0\t2\t0 0 5\n",
      "141\t1\t0.0\t2\t0 1\n",
      "142\t2\t0.0\t2\t0 0 2\n",
      "144\t23\t0.0\t2\t18 3 2\n",
      "146\t2\t0.0\t2\t0 0 2\n",
      "148\t1\t0.0\t2\t0 0 1\n",
      "152\t5\t0.0\t2\t4 0 1\n",
      "154\t1\t0.0\t2\t0 0 1\n",
      "156\t3\t0.0\t2\t1 0 2\n",
      "160\t2\t0.0\t2\t0 0 2\n",
      "162\t1\t0.0\t2\t1\n",
      "164\t1\t0.0\t2\t0 0 1\n",
      "165\t2\t0.0\t2\t2\n",
      "167\t1\t0.0\t2\t0 0 1\n",
      "169\t4\t0.0\t2\t1 2 1\n",
      "172\t1\t0.0\t2\t0 1\n",
      "173\t2\t0.0\t2\t2\n",
      "175\t2\t0.0\t2\t2\n",
      "177\t3\t0.0\t2\t3\n",
      "178\t2\t0.0\t2\t0 0 2\n",
      "179\t1\t0.0\t2\t1\n",
      "180\t1\t0.0\t2\t0 0 1\n",
      "182\t2\t0.0\t2\t0 2\n",
      "184\t3\t0.0\t2\t0 1 2\n",
      "186\t3\t0.0\t2\t0 2 1\n",
      "187\t1\t0.0\t2\t0 1\n",
      "188\t3\t0.0\t2\t2 1\n",
      "189\t1\t0.0\t2\t0 1\n",
      "191\t1\t0.0\t2\t0 1\n",
      "198\t22\t0.0\t2\t0 7 15\n",
      "200\t2\t0.0\t2\t1 1\n",
      "201\t4\t0.0\t2\t0 3 1\n",
      "203\t6\t0.0\t2\t0 0 6\n",
      "205\t2\t0.0\t2\t0 2\n",
      "206\t1\t0.0\t2\t0 0 1\n",
      "207\t3\t0.0\t2\t0 0 3\n",
      "208\t1\t0.0\t2\t0 0 1\n",
      "209\t7\t0.0\t2\t0 0 7\n",
      "211\t3\t0.0\t2\t2 0 1\n",
      "212\t1\t0.0\t2\t1\n",
      "213\t1\t0.0\t2\t1\n",
      "214\t3\t0.0\t2\t0 1 2\n",
      "216\t1\t0.0\t2\t1\n",
      "217\t1\t0.0\t2\t0 0 1\n",
      "218\t2\t0.0\t2\t1 0 1\n",
      "220\t3\t0.0\t2\t0 2 1\n",
      "221\t6\t0.0\t2\t0 0 6\n",
      "223\t1\t0.0\t2\t1\n",
      "227\t1\t0.0\t2\t0 0 1\n",
      "228\t79\t0.0\t2\t72 5 2\n",
      "230\t1\t0.0\t2\t1\n",
      "231\t2\t0.0\t2\t1 0 1\n",
      "232\t13\t0.0\t2\t9 3 1\n",
      "233\t52\t0.0\t2\t0 0 52\n",
      "235\t3\t0.0\t2\t0 2 1\n",
      "239\t3\t0.0\t2\t1 2\n",
      "240\t1\t0.0\t2\t0 1\n",
      "242\t3\t0.0\t2\t0 0 3\n",
      "245\t1\t0.0\t2\t1\n",
      "247\t5\t0.0\t2\t5\n",
      "248\t8\t0.0\t2\t0 2 6\n",
      "249\t6\t0.0\t2\t3 2 1\n",
      "250\t16\t0.0\t2\t10 6\n",
      "251\t7\t0.0\t2\t0 5 2\n",
      "This is cutadapt 3.4 with Python 3.8.8\n",
      "Command line parameters: -a TTACCGCGGCKGCTGGCAC -o /home/backup_files/raw_reads/AM.CW.EW.2022/modified/read2_noprimer.fastq /home/backup_files/raw_reads/AM.CW.EW.2022/modified/read2_mod.fastq\n",
      "Processing reads on 1 core in single-end mode ...\n",
      "[8<----------] 00:00:45     5,670,518 reads  @      8.0 µs/read;   7.45 M reads/minute\n",
      "Finished in 45.64 s (8 µs/read; 7.45 M reads/minute).\n",
      "\n",
      "=== Summary ===\n",
      "\n",
      "Total reads processed:               5,670,518\n",
      "Reads with adapters:                    15,482 (0.3%)\n",
      "Reads written (passing filters):     5,670,518 (100.0%)\n",
      "\n",
      "Total basepairs processed: 1,423,300,018 bp\n",
      "Total written (filtered):  1,423,160,833 bp (100.0%)\n",
      "\n",
      "=== Adapter 1 ===\n",
      "\n",
      "Sequence: TTACCGCGGCKGCTGGCAC; Type: regular 3'; Length: 19; Trimmed: 15482 times\n",
      "\n",
      "No. of allowed errors:\n",
      "1-9 bp: 0; 10-19 bp: 1\n",
      "\n",
      "Bases preceding removed adapters:\n",
      "  A: 24.1%\n",
      "  C: 47.9%\n",
      "  G: 11.3%\n",
      "  T: 16.7%\n",
      "  none/other: 0.0%\n",
      "\n",
      "Overview of removed sequences\n",
      "length\tcount\texpect\tmax.err\terror counts\n",
      "3\t5358\t88601.8\t0\t5358\n",
      "4\t6884\t22150.5\t0\t6884\n",
      "5\t1046\t5537.6\t0\t1046\n",
      "6\t192\t1384.4\t0\t192\n",
      "7\t106\t346.1\t0\t106\n",
      "8\t35\t86.5\t0\t35\n",
      "9\t24\t21.6\t0\t20 4\n",
      "10\t113\t5.4\t1\t70 43\n",
      "11\t59\t1.4\t1\t41 18\n",
      "12\t23\t0.3\t1\t12 11\n",
      "13\t32\t0.1\t1\t22 10\n",
      "14\t11\t0.0\t1\t5 6\n",
      "15\t12\t0.0\t1\t8 4\n",
      "16\t9\t0.0\t1\t6 3\n",
      "17\t17\t0.0\t1\t8 9\n",
      "18\t33\t0.0\t1\t7 26\n",
      "19\t9\t0.0\t1\t8 1\n",
      "20\t12\t0.0\t1\t7 5\n",
      "21\t10\t0.0\t1\t4 6\n",
      "22\t6\t0.0\t1\t4 2\n",
      "23\t19\t0.0\t1\t11 8\n",
      "24\t21\t0.0\t1\t9 12\n",
      "25\t9\t0.0\t1\t7 2\n",
      "26\t19\t0.0\t1\t15 4\n",
      "27\t88\t0.0\t1\t47 41\n",
      "28\t96\t0.0\t1\t78 18\n",
      "29\t303\t0.0\t1\t230 73\n",
      "30\t275\t0.0\t1\t226 49\n",
      "31\t17\t0.0\t1\t12 5\n",
      "32\t5\t0.0\t1\t5\n",
      "33\t19\t0.0\t1\t13 6\n",
      "34\t10\t0.0\t1\t8 2\n",
      "35\t11\t0.0\t1\t9 2\n",
      "36\t53\t0.0\t1\t39 14\n",
      "37\t3\t0.0\t1\t2 1\n",
      "38\t6\t0.0\t1\t6\n",
      "39\t2\t0.0\t1\t2\n",
      "40\t1\t0.0\t1\t1\n",
      "41\t10\t0.0\t1\t6 4\n",
      "42\t162\t0.0\t1\t111 51\n",
      "43\t2\t0.0\t1\t2\n",
      "44\t11\t0.0\t1\t10 1\n",
      "45\t35\t0.0\t1\t21 14\n",
      "46\t4\t0.0\t1\t3 1\n",
      "47\t6\t0.0\t1\t6\n",
      "48\t9\t0.0\t1\t9\n",
      "49\t8\t0.0\t1\t6 2\n",
      "50\t3\t0.0\t1\t3\n",
      "51\t3\t0.0\t1\t3\n",
      "52\t5\t0.0\t1\t4 1\n",
      "53\t2\t0.0\t1\t1 1\n",
      "54\t1\t0.0\t1\t1\n",
      "55\t1\t0.0\t1\t1\n",
      "57\t1\t0.0\t1\t1\n",
      "58\t1\t0.0\t1\t1\n",
      "59\t8\t0.0\t1\t7 1\n",
      "60\t2\t0.0\t1\t1 1\n",
      "61\t2\t0.0\t1\t2\n",
      "62\t3\t0.0\t1\t3\n",
      "64\t2\t0.0\t1\t2\n",
      "71\t3\t0.0\t1\t2 1\n",
      "72\t1\t0.0\t1\t1\n",
      "76\t2\t0.0\t1\t2\n",
      "77\t2\t0.0\t1\t1 1\n",
      "80\t3\t0.0\t1\t2 1\n",
      "81\t5\t0.0\t1\t2 3\n",
      "82\t2\t0.0\t1\t2\n",
      "84\t4\t0.0\t1\t4\n",
      "85\t10\t0.0\t1\t9 1\n",
      "86\t7\t0.0\t1\t5 2\n",
      "87\t8\t0.0\t1\t7 1\n",
      "89\t1\t0.0\t1\t0 1\n",
      "90\t2\t0.0\t1\t2\n",
      "91\t2\t0.0\t1\t2\n",
      "92\t2\t0.0\t1\t2\n",
      "99\t1\t0.0\t1\t1\n",
      "100\t5\t0.0\t1\t4 1\n",
      "105\t1\t0.0\t1\t1\n",
      "106\t3\t0.0\t1\t2 1\n",
      "107\t1\t0.0\t1\t1\n",
      "110\t2\t0.0\t1\t2\n",
      "112\t2\t0.0\t1\t2\n",
      "117\t1\t0.0\t1\t0 1\n",
      "118\t1\t0.0\t1\t1\n",
      "120\t2\t0.0\t1\t1 1\n",
      "121\t20\t0.0\t1\t18 2\n",
      "124\t2\t0.0\t1\t1 1\n",
      "143\t1\t0.0\t1\t0 1\n",
      "144\t18\t0.0\t1\t14 4\n",
      "152\t4\t0.0\t1\t4\n",
      "156\t1\t0.0\t1\t1\n",
      "169\t2\t0.0\t1\t2\n",
      "173\t2\t0.0\t1\t1 1\n",
      "175\t2\t0.0\t1\t1 1\n",
      "177\t3\t0.0\t1\t2 1\n",
      "179\t1\t0.0\t1\t1\n",
      "186\t2\t0.0\t1\t0 2\n",
      "188\t1\t0.0\t1\t0 1\n",
      "198\t1\t0.0\t1\t0 1\n",
      "200\t1\t0.0\t1\t1\n",
      "208\t5\t0.0\t1\t0 5\n",
      "209\t3\t0.0\t1\t0 3\n",
      "215\t2\t0.0\t1\t0 2\n",
      "216\t1\t0.0\t1\t1\n",
      "217\t1\t0.0\t1\t0 1\n",
      "218\t1\t0.0\t1\t1\n",
      "220\t1\t0.0\t1\t1\n",
      "228\t79\t0.0\t1\t72 7\n",
      "230\t1\t0.0\t1\t0 1\n",
      "231\t1\t0.0\t1\t1\n",
      "232\t10\t0.0\t1\t8 2\n",
      "235\t2\t0.0\t1\t2\n",
      "239\t1\t0.0\t1\t1\n",
      "241\t1\t0.0\t1\t0 1\n",
      "245\t1\t0.0\t1\t1\n",
      "247\t3\t0.0\t1\t2 1\n",
      "248\t1\t0.0\t1\t0 1\n",
      "250\t1\t0.0\t1\t0 1\n",
      "251\t4\t0.0\t1\t0 4\n"
     ]
    }
   ],
   "source": [
    "for library in libraries:\n",
    "    raw = library[2]\n",
    "    read1 = library[3]\n",
    "    read2 = library[4]\n",
    "    \n",
    "    !cutadapt -a ATTAGAWACCCBDGTAGTCC -o $raw/modified/read1_noprimer.fastq $raw/modified/read1_mod.fastq\n",
    "    !cutadapt -a TTACCGCGGCKGCTGGCAC -o $raw/modified/read2_noprimer.fastq $raw/modified/read2_mod.fastq\n",
    "    \n",
    "    # Delete unneeded intermediate files\n",
    "    !rm $raw/modified/read*_mod.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Filter short reads\n",
    "\n",
    "Remove sequences with from primer-removed data with less than 100 bp from all files (too short).\n",
    "\n",
    "SLOW STEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    raw = library[2]\n",
    "    os.system(' '.join([\n",
    "        \"python /home/cassi/scripts/qiime2_filter_shortreads.py\",\n",
    "        raw+\"/modified/\"\n",
    "    ]))\n",
    "    \n",
    "    # Remove unneeded intermediate files\n",
    "    !rm $raw/modified/*_noprimer.fastq\n",
    "    !rm $raw/modified/index*_mod.fastq\n",
    "    \n",
    "    # Recompress modified read files\n",
    "    !pigz $raw/modified/read*_filtered.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Concatenate barcodes\n",
    "\n",
    "This step calls a custom script, \"concatenate_barcodes_qiime2.py\". The script must be shuttled to the command line instead of run directly in jupyter notebooks because jupyter has memory issues that truncates the barcodes file without an error (I think).\n",
    "\n",
    "This script requires your index1 and index2 files to be named index1_mod.fastq and index2_mod.fastq and be located in a directory within the raw read directory called 'modified'. This should have been taken care of in earlier steps in this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    raw = library[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"python /home/cassi/scripts/qiime2_concatenate_barcodes.py\",\n",
    "        raw+\"/modified\"]))\n",
    "  \n",
    "    # Recompress modified index files and newly created barcodes.fastq file\n",
    "    !pigz $raw/modified/*.fastq   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Move raw data to library directories\n",
    "\n",
    "Creates intermediate directory in library directory. All subsequent files except for the final files will be placed there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    proc = library[1]\n",
    "    raw = library[2]\n",
    "    \n",
    "    # Create output directory if it doesn't exist already\n",
    "    if not os.path.isdir(os.path.join(proc, \"intermediate\")):\n",
    "        !mkdir $proc/intermediate\n",
    "    \n",
    "    # Create a symbolic link to the read data\n",
    "    # QIIME2 import requires a directory containing files named: forward.fastq.gz, reverse.fastq.gz and barcodes.fastq.gz \n",
    "    !ln -s $raw/modified/read1_filtered.fastq.gz $proc/intermediate/forward.fastq.gz\n",
    "    !ln -s $raw/modified/read2_filtered.fastq.gz $proc/intermediate/reverse.fastq.gz\n",
    "    \n",
    "    # Move concatenated barcodes to project directory\n",
    "    !cp $raw/modified/barcodes.fastq.gz $proc/intermediate/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Import into QIIME2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime tools import\",\n",
    "        \"--type EMPPairedEndSequences\",\n",
    "        \"--input-path \"+proc+\"/intermediate\",\n",
    "        \"--output-path \"+proc+\"/intermediate/\"+name+\".qza\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Demultiplex\n",
    "\n",
    "The barcode you supply to QIIME is now a concatenation of your forward and reverse barcode. Your 'forward' barcode is actually the reverse complement of your reverse barcode and the 'reverse' is your forward barcode.\n",
    "\n",
    "Slow step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    metadata = library[7]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux emp-paired\",\n",
    "        \"--m-barcodes-file \"+proc+\"/\"+metadata,\n",
    "        \"--m-barcodes-column BarcodeSequence\",\n",
    "        \"--p-no-golay-error-correction\",\n",
    "        \"--i-seqs \"+proc+\"/intermediate/\"+name+\".qza\",\n",
    "        \"--o-per-sample-sequences \"+proc+\"/intermediate/\"+name+\".demux\",\n",
    "        \"--o-error-correction-details \"+proc+\"/intermediate/\"+name+\".demux-details.qza\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Visualize quality scores\n",
    "\n",
    "Drop output from below command into https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux summarize\",\n",
    "        \"--i-data \"+proc+\"/intermediate/\"+name+\".demux.qza\",\n",
    "        \"--o-visualization \"+proc+\"/intermediate/\"+name+\".demux.qzv\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Trimming parameters | USER INPUT REQUIRED\n",
    "\n",
    "Based on the quality scores of the bp along the reads, choose trim and truncate values for the forward and reverse reads. Trim refers to the start of a sequence and truncate the total length (i.e. number of bases to remove from end).\n",
    "\n",
    "All trimming parameters must be the same for datasets that will be directly compared to one-another because ASVs are determined, in part, by sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input your trimming parameters into a python dictionary for all libraries\n",
    "# trim_dict = {\"LibraryName1\":[trim_forward, truncate_forward, trim_reverse, truncate_reverse],\n",
    "#              \"LibraryName2\":[trim_forward, truncate_forward, trim_reverse, truncate_reverse],\n",
    "#               etc...}\n",
    "\n",
    "# The example in the Atacama Desert Tutorial trims 13 bp from the start of each read and does not remove any bases from the end of the 150 bp reads:\n",
    "#  --p-trim-left-f 13 \\  \n",
    "#  --p-trim-left-r 13 \\\n",
    "#  --p-trunc-len-f 150 \\\n",
    "#  --p-trunc-len-r 150\n",
    "\n",
    "trim_dict = {\"NIFA_micro2\":[10,200,10,200]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Trim, denoise and join (aka 'merge') reads using DADA2\n",
    "\n",
    "See the [QIIME2 dada2 denoise-paired documentation](https://docs.qiime2.org/2021.8/plugins/available/dada2/denoise-paired/) for the default parameters used.\n",
    "\n",
    "Slow step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime dada2 denoise-paired\",\n",
    "        \"--i-demultiplexed-seqs \"+proc+\"/intermediate/\"+name+\".demux.qza\",\n",
    "        \"--o-table \"+proc+\"/intermediate/\"+name+\".table.qza\",\n",
    "        \"--o-representative-sequences \"+proc+\"/intermediate/\"+name+\".rep-seqs.qza\",\n",
    "        \"--o-denoising-stats \"+proc+\"/intermediate/\"+name+\".denoising-stats.qza\",\n",
    "        \"--p-trim-left-f \"+str(trim_dict[name][0]),\n",
    "        \"--p-trim-left-r \"+str(trim_dict[name][2]),\n",
    "        \"--p-trunc-len-f \"+str(trim_dict[name][1]),\n",
    "        \"--p-trunc-len-r \"+str(trim_dict[name][3]),\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Create summary of ASVs\n",
    "\n",
    "Drop outputs from below command into https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    metadata = library[7]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table summarize\",\n",
    "        \"--i-table \"+proc+\"/intermediate/\"+name+\".table.qza\",\n",
    "        \"--o-visualization \"+proc+\"/intermediate/\"+name+\".table.qzv\",\n",
    "        \"--m-sample-metadata-file \"+proc+\"/\"+metadata\n",
    "    ]))\n",
    "\n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table tabulate-seqs\",\n",
    "        \"--i-data \"+proc+\"/intermediate/\"+name+\".rep-seqs.qza\",\n",
    "        \"--o-visualization \"+proc+\"/intermediate/\"+name+\".rep-seqs.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13: Classify sequences\n",
    "\n",
    "Different QIIME2 versions can conflict with certain classifier database versions. This section will likely need to be updated. Download the latest classifiers here: https://docs.qiime2.org/2021.4/data-resources/\n",
    "\n",
    "* Using SILVA v138 pre-built classifier trained on scikit learn 0.24.1.\n",
    "\n",
    "View output in https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use classifier for chosen database\n",
    "try:\n",
    "    if db == \"GreenGenes\":\n",
    "        classifier_db = \"/home/db/GreenGenes/qiime2_13.8.99_515.806_nb.classifier.qza\" # out of date\n",
    "    else:\n",
    "        classifier_db = \"~/databases/silva/silva-138-99-515-806-nb-classifier.qza\"\n",
    "except:\n",
    "        classifier_db = \"~/databases/silva/silva-138-99-515-806-nb-classifier.qza\"\n",
    "        \n",
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    metadata = library[7]\n",
    "    domain = library[8]\n",
    "\n",
    "    # Classify\n",
    "    if domain == 'bacteria':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier\",\n",
    "            classifier_db,\n",
    "            \"--i-reads \"+proc+\"/intermediate/\"+name+\".rep-seqs.qza\",\n",
    "            \"--o-classification \"+proc+\"/intermediate/\"+name+\".taxonomy.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "    if domain == 'fungi':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier /home/db/UNITE/qiime2_unite_ver7.99_20.11.2016_classifier.qza\", # out of date\n",
    "            \"--i-reads \"+proc+\"/intermediate/\"+name+\".rep-seqs.qza\",\n",
    "            \"--o-classification \"+proc+\"/intermediate/\"+name+\".taxonomy.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "    # Output summary\n",
    "    os.system(' '.join([\n",
    "        \"qiime metadata tabulate\",\n",
    "        \"--m-input-file \"+proc+\"/intermediate/\"+name+\".taxonomy.qza\",\n",
    "        \"--o-visualization \"+proc+\"/intermediate/\"+name+\".taxonomy-summary.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 14: Combine representative sequences for tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bacteria\n",
    "\n",
    "# Create representative sequences file\n",
    "repseqsbac = []\n",
    "\n",
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    domain = library[8]\n",
    "    \n",
    "    # Create list of rep seq files\n",
    "    if domain == \"bacteria\":\n",
    "        repseqsbac.append(\"--i-data \" + os.path.join(proc, \"intermediate\", name+\".rep-seqs.qza\"))\n",
    "\n",
    "# Create tree directory\n",
    "if not os.path.isdir(os.path.join(project, \"tree\")):\n",
    "    !mkdir $project/tree\n",
    "    \n",
    "# Merge rep sequences from all bacterial libraries for tree\n",
    "os.system(' '.join([\n",
    "    \"qiime feature-table merge-seqs\",\n",
    "    \" \".join(repseqsbac),\n",
    "    \"--o-merged-data \"+project+\"/tree/\"+treename+\".rep-seqs-merged.qza\"\n",
    "]))\n",
    "\n",
    "# Fungi\n",
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    domain = library[8]\n",
    "    \n",
    "    # Create representative sequences file\n",
    "    repseqsfung = []\n",
    "    \n",
    "    if domain == \"fungi\":\n",
    "        repseqsfung.append(\"--i-data \" + os.path.join(proc, \"intermediate\", name+\".rep-seqs.qza\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 15: Make phylogenetic tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    domain = library[8]\n",
    "\n",
    "    if domain == \"bacteria\":\n",
    "        # Generate alignment with MAFFT\n",
    "        os.system(' '.join([\n",
    "            \"qiime alignment mafft\",\n",
    "            \"--i-sequences \"+project+\"/tree/\"+treename+\".rep-seqs-merged.qza\",\n",
    "            \"--o-alignment \"+project+\"/tree/\"+treename+\".rep-seqs-merged-aligned.qza\",\n",
    "            \"--p-n-threads\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "        # Mask hypervariable regions in alignment\n",
    "        os.system(' '.join([\n",
    "            \"qiime alignment mask\",\n",
    "            \"--i-alignment \"+project+\"/tree/\"+treename+\".rep-seqs-merged-aligned.qza\",\n",
    "            \"--o-masked-alignment \"+project+\"/tree/\"+treename+\".rep-seqs-merged-aligned-masked.qza\",\n",
    "        ]))\n",
    "\n",
    "        # Generate tree with FastTree\n",
    "        os.system(' '.join([\n",
    "            \"qiime phylogeny fasttree\",\n",
    "            \"--i-alignment \"+project+\"/tree/\"+treename+\".rep-seqs-merged-aligned-masked.qza\",\n",
    "            \"--o-tree \"+project+\"/tree/\"+treename+\".tree-unrooted.qza\",\n",
    "            \"--p-n-threads\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "        # Root the tree\n",
    "        os.system(' '.join([\n",
    "            \"qiime phylogeny midpoint-root\",\n",
    "            \"--i-tree \"+project+\"/tree/\"+treename+\".tree-unrooted.qza\",\n",
    "            \"--o-rooted-tree \"+project+\"/tree/\"+treename+\".tree-rooted.qza\"\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 16: Reformat taxonomy\n",
    "\n",
    "Define function to tidy the taxonomy and make it compatible with phyloseq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_taxonomy(tax_dirty, min_support):\n",
    "    output = open(re.sub(\".tsv\",\"-fixed.tsv\",tax_dirty), \"w\")\n",
    "    \n",
    "    full_rank_length = 7\n",
    "    output.write(\"\\t\".join([\"ASV\",\"Domain\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\",\"Species\"])+\"\\n\")\n",
    "\n",
    "    with open(tax_dirty, \"r\") as f:\n",
    "        next(f)\n",
    "\n",
    "        for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "\n",
    "                read_id = line[0]\n",
    "                tax_string = line[1]\n",
    "                \n",
    "                ## Remove taxonomy prefixes and underscores (only coded for Silva classifications so far)\n",
    "                if db == \"Silva\":\n",
    "                    tax_string = re.sub(\"[a-z]__\", \"\", tax_string)\n",
    "                    tax_string = re.sub(\"_\", \" \", tax_string)\n",
    "\n",
    "                # Split full rank into ranks\n",
    "                full_rank = tax_string.split(\";\")\n",
    "\n",
    "                ## Identify the lowest classified taxonomic rank\n",
    "                # Account for cases when a taxonomic rank contains an empty space (common in GreenGenes output)\n",
    "                last_classified = full_rank[len(full_rank)-1]            \n",
    "\n",
    "                count = 1\n",
    "                while last_classified == \" \":\n",
    "                    last_classified = full_rank[len(full_rank)-count]\n",
    "                    count = count + 1\n",
    "\n",
    "                # Annotate the last classified as 'putative' if it does not meet the minimum support criteria\n",
    "                if float(line[2]) < float(min_support):\n",
    "                        full_rank[full_rank.index(last_classified)] = \"putative \"+last_classified\n",
    "                        last_classified = \"putative \"+last_classified\n",
    "\n",
    "                # Add in columns containing unclassified taxonomic information\n",
    "                for n in range(full_rank.index(last_classified)+1, full_rank_length, 1):               \n",
    "                    try:\n",
    "                        full_rank[n] = \"unclassified \"+last_classified\n",
    "                    except:\n",
    "                        full_rank.append(\"unclassified \"+last_classified)\n",
    "\n",
    "                # Write taxonomy to file\n",
    "                output.write(read_id+\"\\t\"+'\\t'.join(full_rank)+\"\\n\")\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 17: Export from QIIME2\n",
    "\n",
    "All final files will be placed in 'final' directory in library directories. Final tree will be in a 'tree' directory in the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in libraries:\n",
    "    name = library[0]\n",
    "    proc = library[1]\n",
    "    metadata = library[7]\n",
    "\n",
    "    # Final output paths\n",
    "    fasta_final = proc+\"/final/\"+name+\".rep-seqs-final.fasta\"\n",
    "    tax_final= proc+\"/final/\"+name+\".taxonomy-final.tsv\"\n",
    "    count_final = proc+\"/final/\"+name+\".counts-final.biom\"\n",
    "    \n",
    "    # Make final data directories\n",
    "    if not os.path.isdir(os.path.join(proc, \"final\")):\n",
    "        !mkdir $proc/final\n",
    "        \n",
    "    # Export ASV table\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+proc+\"/intermediate/\"+name+\".table.qza\",\n",
    "        \"--output-path \"+proc+\"/intermediate/\"\n",
    "    ]))\n",
    "\n",
    "    # Export taxonomic classifications\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+proc+\"/intermediate/\"+name+\".taxonomy.qza\",\n",
    "        \"--output-path \"+proc+\"/intermediate/\"\n",
    "    ]))\n",
    "    \n",
    "    # Reformat classifications  \n",
    "    format_taxonomy(proc+\"/intermediate/taxonomy.tsv\", min_support)\n",
    "\n",
    "    # Export representative sequences\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+proc+\"/intermediate/\"+name+\".rep-seqs.qza\",\n",
    "        \"--output-path \"+proc+\"/intermediate/\"\n",
    "    ]))\n",
    "    \n",
    "    # Rename exported files and move to final directory\n",
    "    !mv $proc/intermediate/dna-sequences.fasta $fasta_final\n",
    "    !mv $proc/intermediate/feature-table.biom $count_final\n",
    "    !mv $proc/intermediate/taxonomy-fixed.tsv $tax_final\n",
    "    \n",
    "    # Reformat count table\n",
    "    tmp_tsv = re.sub(name+\".counts-final.biom\", \"tmp.tsv\", count_final)\n",
    "    tmp2_tsv = re.sub(name+\".counts-final.biom\", \"tmp2.tsv\", count_final)\n",
    "    count_tsv = re.sub(\".biom\", \".tsv\", count_final)\n",
    "    !biom convert -i $count_final -o $tmp_tsv --to-tsv # conver to .tsv\n",
    "    !tail -n +2 $tmp_tsv > $tmp2_tsv # remove header\n",
    "    !sed 's/\\#OTU ID/ASV/g' $tmp2_tsv > $count_tsv # replace OTU with ASV\n",
    "    !rm $tmp_tsv\n",
    "    !rm $tmp2_tsv\n",
    "    \n",
    "# Export tree\n",
    "os.system(' '.join([\n",
    "    \"qiime tools export\",\n",
    "    \"--input-path \"+project+\"/tree/\"+treename+\".tree-rooted.qza\",\n",
    "    \"--output-path \"+project+\"/tree/\"\n",
    "]))\n",
    "\n",
    "# Rename tree file\n",
    "tree_final = project+\"/tree/\"+treename+\".tree-final.nwk\"\n",
    "!mv $project/tree/\"tree.nwk\" $tree_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export tree\n",
    "os.system(' '.join([\n",
    "    \"qiime tools export\",\n",
    "    \"--input-path \"+project+\"/tree/\"+treename+\".tree-rooted.qza\",\n",
    "    \"--output-path \"+project+\"/tree/\"\n",
    "]))\n",
    "\n",
    "# Rename tree file\n",
    "tree_final = project+\"/tree/\"+treename+\".tree-final.nwk\"\n",
    "!mv $project/tree/\"tree.nwk\" $tree_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export merged rep seqs\n",
    "os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+project+\"/tree/\"+treename+\".rep-seqs-merged.qza\",\n",
    "        \"--output-path \"+project+\"/tree/\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
